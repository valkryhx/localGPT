{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-05T11:41:36.879391Z","iopub.execute_input":"2023-09-05T11:41:36.879923Z","iopub.status.idle":"2023-09-05T11:41:36.892224Z","shell.execute_reply.started":"2023-09-05T11:41:36.879876Z","shell.execute_reply":"2023-09-05T11:41:36.891163Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/localGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd localGPT/\n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat /kaggle/working/localGPT/constants.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 上面这个max_se_length 其实是https://github.com/langchain-ai/langchain/blob/0689628489967785f3a11a9f29d8f6f90930f4f4/libs/langchain/langchain/embeddings/huggingface.py#L231C9-L231C65\nBreadcrumbslangchain/libs/langchain/langchain/embeddings\n/huggingface.py 231行sentence_transformers.SentenceTransformer 加载SentenceTransformer 默认的512  要动的话需要动源码","metadata":{}},{"cell_type":"markdown","source":"# ls DB/  ingest 之后的向量文件在这里  如果不想要就 rm -rf DB 那下次就要重新ingest","metadata":{}},{"cell_type":"code","source":"#run_localGPT.py 中的query=input()无法在kaggle环境上正常执行 我们自己把代码拿出来跑\n#!python run_localGPT.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# codes are just from local_GPT.py\nimport logging\n\nimport click\nimport torch\nfrom auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h=[(\"123\"),(\"3\")]\nsum([len(item) for item in h])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity","metadata":{}},{"cell_type":"code","source":"\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = Chroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n    )\n#retriever = db.as_retriever()\n\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":0.75}\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n    就回答不知道，不要编造答案.\n\n    {context}\n\n    {history}\n    问: {question}\n    答:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\nmemory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        print(\"12345\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n            for document in docs:\n                print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n                print(document.page_content)\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Chroma和FAISS默认的相似度计算metric是L2 distance\n在Chroma.from_documents方法中加入参数\ncollection_metadata={\"hnsw:space\": \"cosine\"} 改成cosine 相似度 在[0,1]之间 越高说明两个向量越相似\n\nIs you are using Chroma, you should set the distance metric when creating a collection: https://docs.trychroma.com/usage-guide#changing-the-distance-function\n\nThe default distance is l2. That is why for me it used to give scores like 3626.016357421875 when using the function similarity_search_with_relevance_scores(). On changing it to cosine, the scores are now between (0, 1] with scores closer to 1 depicting higher similarity.\n\nChroma.from_documents(documents=documents, embedding=cohere, collection_metadata={\"hnsw:space\": \"cosine\"})\n\n参考 https://stackoverflow.com/questions/76678783/langchains-chroma-vectordb-similarity-search-with-score-and-vectordb-simil\n参考 langchain.vectorstors.Chroma 源码  https://api.python.langchain.com/en/latest/_modules/langchain/vectorstores/chroma.html\n的class Chroma类的__init__方法","metadata":{}},{"cell_type":"markdown","source":"# https://github.com/langchain-ai/langchain/issues/6481\n# https://docs.trychroma.com/usage-guide#changing-the-distance-function\n#注意这个 https://stackoverflow.com/questions/76678783/langchains-chroma-vectordb-similarity-search-with-score-and-vectordb-simil\n# https://github.com/langchain-ai/langchain/issues/5458  \n这个写了怎么在向量计算结果中加入相似度阈值和个数阈值进行过滤 结果不超过k个 并且score_threshold要大于这个0.5才参与候选\nretriever=db.as_retriever(search_type=\"similarity_score_threshold\", \n                          search_kwargs={\"k\":3, \"score_threshold\":0.5})    \nhttps://github.com/langchain-ai/langchain/blob/e60e1cdf23ad73b2e0a40034c0ddfc3c8b0c9c4d/libs/langchain/langchain/vectorstores/base.py#L460","metadata":{}},{"cell_type":"markdown","source":"# langchain使用样例\nhttps://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa","metadata":{}},{"cell_type":"markdown","source":"# python 合并字典 优雅\nhttps://segmentfault.com/a/1190000010567015","metadata":{}},{"cell_type":"code","source":"# 用新版本的https://github.com/valkryhx/localGPT   \n# branch localGPT_0831_langchain_v02","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf /kaggle/working/localGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>注意 要让Chroma使用cosine distance（注意不是cosine similarity）而非默认的L2 distance</font>  \nhttps://github.com/valkryhx/localGPT/blob/localGPT_0831_langchain_v02/ingest.py#L150 加一行   collection_metadata={\"hnsw:space\": \"cosine\"},\n在下面的# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"ip\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n也加一行\n\n# 魔改了langchain/vectorstores/chroma.py 增加了distance value输出  越相似的distance越小\n# #retriever = db.as_retriever() 改了 增加 search_type=\"similarity_score_threshold\", search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )\n # 注意 这个threshold如果取0.7 那么实际对应的是distance value小于0.3的text 也就是说distance越小 则1-distance 越大 越容易超过threshold  这符合逻辑和变量名定义  后面print时 我会用1-distance的值来显示","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b localGPT_0831_langchain_v02 https://github.com/valkryhx/localGPT  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd localGPT\n!git status","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 重新生成   \n#ls DB\n#!rm -rf DB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# codes are just from local_GPT.py\nimport logging\n\nimport click\nimport torch\n#from auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"根据这个问题\n\nhttps://github.com/langchain-ai/langchain/issues/4710\n\nhttps://github.com/langchain-ai/langchain/issues/5416\n\n改的_results_to_docs_and_scores方法中\nmetadata={**result[1],**{\"distances\":result[2]} } or {})","metadata":{}},{"cell_type":"code","source":"\"\"\"Wrapper around ChromaDB embeddings platform.\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport uuid\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n)\n\nimport numpy as np\n\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.utils import xor_args\nfrom langchain.vectorstores.base import VectorStore\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\n\nif TYPE_CHECKING:\n    import chromadb\n    import chromadb.config\n    from chromadb.api.types import ID, OneOrMany, Where, WhereDocument\n\nlogger = logging.getLogger()\nDEFAULT_K = 4  # Number of Documents to return.\n\n\ndef _results_to_docs(results: Any) -> List[Document]:\n    return [doc for doc, _ in _results_to_docs_and_scores(results)]\n\n\ndef _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\n    return [\n        # TODO: Chroma can do batch querying,\n        # we shouldn't hard code to the 1st result\n        \n        # merge two dicts ,metadata = {**d1,**d2}\n        (Document(page_content=result[0], metadata={**result[1],**{\"distances\":result[2]} } or {}), result[2])\n        for result in zip(\n            results[\"documents\"][0],\n            results[\"metadatas\"][0],\n            results[\"distances\"][0],\n        )\n    ]\n\n\nclass MyChroma(VectorStore):\n    \"\"\"Wrapper around ChromaDB embeddings platform.\n\n    To use, you should have the ``chromadb`` python package installed.\n\n    Example:\n        .. code-block:: python\n\n                from langchain.vectorstores import Chroma\n                from langchain.embeddings.openai import OpenAIEmbeddings\n\n                embeddings = OpenAIEmbeddings()\n                vectorstore = Chroma(\"langchain_store\", embeddings)\n    \"\"\"\n\n    _LANGCHAIN_DEFAULT_COLLECTION_NAME = \"langchain\"\n\n    def __init__(\n        self,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        embedding_function: Optional[Embeddings] = None,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        collection_metadata: Optional[Dict] = None,\n        client: Optional[chromadb.Client] = None,\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\n    ) -> None:\n        \"\"\"Initialize with Chroma client.\"\"\"\n        try:\n            import chromadb\n            import chromadb.config\n        except ImportError:\n            raise ValueError(\n                \"Could not import chromadb python package. \"\n                \"Please install it with `pip install chromadb`.\"\n            )\n\n        if client is not None:\n            self._client_settings = client_settings\n            self._client = client\n            self._persist_directory = persist_directory\n        else:\n            if client_settings:\n                # If client_settings is provided with persist_directory specified,\n                # then it is \"in-memory and persisting to disk\" mode.\n                client_settings.persist_directory = (\n                    persist_directory or client_settings.persist_directory\n                )\n                if client_settings.persist_directory is not None:\n                    # Maintain backwards compatibility with chromadb < 0.4.0\n                    major, minor, _ = chromadb.__version__.split(\".\")\n                    if int(major) == 0 and int(minor) < 4:\n                        client_settings.chroma_db_impl = \"duckdb+parquet\"\n\n                _client_settings = client_settings\n            elif persist_directory:\n                # Maintain backwards compatibility with chromadb < 0.4.0\n                major, minor, _ = chromadb.__version__.split(\".\")\n                if int(major) == 0 and int(minor) < 4:\n                    _client_settings = chromadb.config.Settings(\n                        chroma_db_impl=\"duckdb+parquet\",\n                    )\n                else:\n                    _client_settings = chromadb.config.Settings(is_persistent=True)\n                _client_settings.persist_directory = persist_directory\n            else:\n                _client_settings = chromadb.config.Settings()\n            self._client_settings = _client_settings\n            self._client = chromadb.Client(_client_settings)\n            self._persist_directory = (\n                _client_settings.persist_directory or persist_directory\n            )\n\n        self._embedding_function = embedding_function\n        self._collection = self._client.get_or_create_collection(\n            name=collection_name,\n            embedding_function=self._embedding_function.embed_documents\n            if self._embedding_function is not None\n            else None,\n            metadata=collection_metadata,\n        )\n        self.override_relevance_score_fn = relevance_score_fn\n\n    @property\n    def embeddings(self) -> Optional[Embeddings]:\n        return self._embedding_function\n\n    @xor_args((\"query_texts\", \"query_embeddings\"))\n    def __query_collection(\n        self,\n        query_texts: Optional[List[str]] = None,\n        query_embeddings: Optional[List[List[float]]] = None,\n        n_results: int = 4,\n        where: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Query the chroma collection.\"\"\"\n        try:\n            import chromadb  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"Could not import chromadb python package. \"\n                \"Please install it with `pip install chromadb`.\"\n            )\n        return self._collection.query(\n            query_texts=query_texts,\n            query_embeddings=query_embeddings,\n            n_results=n_results,\n            where=where,\n            **kwargs,\n        )\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            ids (Optional[List[str]], optional): Optional list of IDs.\n\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        # TODO: Handle the case where the user doesn't provide ids on the Collection\n        if ids is None:\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n        texts = list(texts)\n        if self._embedding_function is not None:\n            embeddings = self._embedding_function.embed_documents(texts)\n        if metadatas:\n            # fill metadatas with empty dicts if somebody\n            # did not specify metadata for all texts\n            length_diff = len(texts) - len(metadatas)\n            if length_diff:\n                metadatas = metadatas + [{}] * length_diff\n            empty_ids = []\n            non_empty_ids = []\n            for idx, m in enumerate(metadatas):\n                if m:\n                    non_empty_ids.append(idx)\n                else:\n                    empty_ids.append(idx)\n            if non_empty_ids:\n                metadatas = [metadatas[idx] for idx in non_empty_ids]\n                texts_with_metadatas = [texts[idx] for idx in non_empty_ids]\n                embeddings_with_metadatas = (\n                    [embeddings[idx] for idx in non_empty_ids] if embeddings else None\n                )\n                ids_with_metadata = [ids[idx] for idx in non_empty_ids]\n                try:\n                    self._collection.upsert(\n                        metadatas=metadatas,\n                        embeddings=embeddings_with_metadatas,\n                        documents=texts_with_metadatas,\n                        ids=ids_with_metadata,\n                    )\n                except ValueError as e:\n                    if \"Expected metadata value to be\" in str(e):\n                        msg = (\n                            \"Try filtering complex metadata from the document using \"\n                            \"langchain.vectorstore.utils.filter_complex_metadata.\"\n                        )\n                        raise ValueError(e.args[0] + \"\\n\\n\" + msg)\n                    else:\n                        raise e\n            if empty_ids:\n                texts_without_metadatas = [texts[j] for j in empty_ids]\n                embeddings_without_metadatas = (\n                    [embeddings[j] for j in empty_ids] if embeddings else None\n                )\n                ids_without_metadatas = [ids[j] for j in empty_ids]\n                self._collection.upsert(\n                    embeddings=embeddings_without_metadatas,\n                    documents=texts_without_metadatas,\n                    ids=ids_without_metadatas,\n                )\n        else:\n            self._collection.upsert(\n                embeddings=embeddings,\n                documents=texts,\n                ids=ids,\n            )\n        return ids\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Run similarity search with Chroma.\n\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\n        docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\n        return [doc for doc, _ in docs_and_scores]\n\n    def similarity_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to embedding vector.\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List of Documents most similar to the query vector.\n        \"\"\"\n        results = self.__query_collection(\n            query_embeddings=embedding, n_results=k, where=filter\n        )\n        return _results_to_docs(results)\n\n    def similarity_search_by_vector_with_relevance_scores(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"\n        Return docs most similar to embedding vector and similarity score.\n\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        results = self.__query_collection(\n            query_embeddings=embedding, n_results=k, where=filter\n        )\n        return _results_to_docs_and_scores(results)\n\n    def similarity_search_with_score(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Run similarity search with Chroma with distance.\n\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        if self._embedding_function is None:\n            results = self.__query_collection(\n                query_texts=[query], n_results=k, where=filter\n            )\n        else:\n            query_embedding = self._embedding_function.embed_query(query)\n            results = self.__query_collection(\n                query_embeddings=[query_embedding], n_results=k, where=filter\n            )\n\n        return _results_to_docs_and_scores(results)\n\n    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n        \"\"\"\n        The 'correct' relevance function\n        may differ depending on a few things, including:\n        - the distance / similarity metric used by the VectorStore\n        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)\n        - embedding dimensionality\n        - etc.\n        \"\"\"\n        if self.override_relevance_score_fn:\n            return self.override_relevance_score_fn\n\n        distance = \"l2\"\n        distance_key = \"hnsw:space\"\n        metadata = self._collection.metadata\n\n        if metadata and distance_key in metadata:\n            distance = metadata[distance_key]\n\n        if distance == \"cosine\":\n            return self._cosine_relevance_score_fn\n        elif distance == \"l2\":\n            return self._euclidean_relevance_score_fn\n        elif distance == \"ip\":\n            return self._max_inner_product_relevance_score_fn\n        else:\n            raise ValueError(\n                \"No supported normalization function\"\n                f\" for distance metric of type: {distance}.\"\n                \"Consider providing relevance_score_fn to Chroma constructor.\"\n            )\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n\n        results = self.__query_collection(\n            query_embeddings=embedding,\n            n_results=fetch_k,\n            where=filter,\n            include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"],\n        )\n        mmr_selected = maximal_marginal_relevance(\n            np.array(embedding, dtype=np.float32),\n            results[\"embeddings\"][0],\n            k=k,\n            lambda_mult=lambda_mult,\n        )\n\n        candidates = _results_to_docs(results)\n\n        selected_results = [r for i, r in enumerate(candidates) if i in mmr_selected]\n        return selected_results\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            query: Text to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        if self._embedding_function is None:\n            raise ValueError(\n                \"For MMR search, you must specify an embedding function on\" \"creation.\"\n            )\n\n        embedding = self._embedding_function.embed_query(query)\n        docs = self.max_marginal_relevance_search_by_vector(\n            embedding, k, fetch_k, lambda_mult=lambda_mult, filter=filter\n        )\n        return docs\n\n    def delete_collection(self) -> None:\n        \"\"\"Delete the collection.\"\"\"\n        self._client.delete_collection(self._collection.name)\n\n    def get(\n        self,\n        ids: Optional[OneOrMany[ID]] = None,\n        where: Optional[Where] = None,\n        limit: Optional[int] = None,\n        offset: Optional[int] = None,\n        where_document: Optional[WhereDocument] = None,\n        include: Optional[List[str]] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Gets the collection.\n\n        Args:\n            ids: The ids of the embeddings to get. Optional.\n            where: A Where type dict used to filter results by.\n                   E.g. `{\"color\" : \"red\", \"price\": 4.20}`. Optional.\n            limit: The number of documents to return. Optional.\n            offset: The offset to start returning results from.\n                    Useful for paging results with limit. Optional.\n            where_document: A WhereDocument type dict used to filter by the documents.\n                            E.g. `{$contains: {\"text\": \"hello\"}}`. Optional.\n            include: A list of what to include in the results.\n                     Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n                     Ids are always included.\n                     Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n        \"\"\"\n        kwargs = {\n            \"ids\": ids,\n            \"where\": where,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"where_document\": where_document,\n        }\n\n        if include is not None:\n            kwargs[\"include\"] = include\n\n        return self._collection.get(**kwargs)\n\n    def persist(self) -> None:\n        \"\"\"Persist the collection.\n\n        This can be used to explicitly persist the data to disk.\n        It will also be called automatically when the object is destroyed.\n        \"\"\"\n        if self._persist_directory is None:\n            raise ValueError(\n                \"You must specify a persist_directory on\"\n                \"creation to persist the collection.\"\n            )\n        import chromadb\n\n        # Maintain backwards compatibility with chromadb < 0.4.0\n        major, minor, _ = chromadb.__version__.split(\".\")\n        if int(major) == 0 and int(minor) < 4:\n            self._client.persist()\n\n    def update_document(self, document_id: str, document: Document) -> None:\n        \"\"\"Update a document in the collection.\n\n        Args:\n            document_id (str): ID of the document to update.\n            document (Document): Document to update.\n        \"\"\"\n        text = document.page_content\n        metadata = document.metadata\n        if self._embedding_function is None:\n            raise ValueError(\n                \"For update, you must specify an embedding function on creation.\"\n            )\n        embeddings = self._embedding_function.embed_documents([text])\n\n        self._collection.update(\n            ids=[document_id],\n            embeddings=embeddings,\n            documents=[text],\n            metadatas=[metadata],\n        )\n\n    @classmethod\n    def from_texts(\n        cls: Type[Chroma],\n        texts: List[str],\n        embedding: Optional[Embeddings] = None,\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        client: Optional[chromadb.Client] = None,\n        collection_metadata: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> Chroma:\n        \"\"\"Create a Chroma vectorstore from a raw documents.\n\n        If a persist_directory is specified, the collection will be persisted there.\n        Otherwise, the data will be ephemeral in-memory.\n\n        Args:\n            texts (List[str]): List of texts to add to the collection.\n            collection_name (str): Name of the collection to create.\n            persist_directory (Optional[str]): Directory to persist the collection.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n            collection_metadata (Optional[Dict]): Collection configurations.\n                                                  Defaults to None.\n\n        Returns:\n            Chroma: Chroma vectorstore.\n        \"\"\"\n        chroma_collection = cls(\n            collection_name=collection_name,\n            embedding_function=embedding,\n            persist_directory=persist_directory,\n            client_settings=client_settings,\n            client=client,\n            collection_metadata=collection_metadata,\n            **kwargs,\n        )\n        chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n        return chroma_collection\n\n    @classmethod\n    def from_documents(\n        cls: Type[Chroma],\n        documents: List[Document],\n        embedding: Optional[Embeddings] = None,\n        ids: Optional[List[str]] = None,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        client: Optional[chromadb.Client] = None,  # Add this line\n        collection_metadata: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> Chroma:\n        \"\"\"Create a Chroma vectorstore from a list of documents.\n\n        If a persist_directory is specified, the collection will be persisted there.\n        Otherwise, the data will be ephemeral in-memory.\n\n        Args:\n            collection_name (str): Name of the collection to create.\n            persist_directory (Optional[str]): Directory to persist the collection.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            documents (List[Document]): List of documents to add to the vectorstore.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n            collection_metadata (Optional[Dict]): Collection configurations.\n                                                  Defaults to None.\n\n        Returns:\n            Chroma: Chroma vectorstore.\n        \"\"\"\n        texts = [doc.page_content for doc in documents]\n        metadatas = [doc.metadata for doc in documents]\n        return cls.from_texts(\n            texts=texts,\n            embedding=embedding,\n            metadatas=metadatas,\n            ids=ids,\n            collection_name=collection_name,\n            persist_directory=persist_directory,\n            client_settings=client_settings,\n            client=client,\n            collection_metadata=collection_metadata,\n            **kwargs,\n        )\n\n    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:\n        \"\"\"Delete by vector IDs.\n\n        Args:\n            ids: List of ids to delete.\n        \"\"\"\n        self._collection.delete(ids=ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>参考3中distance定义 https://github.com/nmslib/hnswlib/tree/master#python-bindings</font>\n# Distance\tparameter\tEquation\n# Squared L2\t'l2'\td = sum((Ai-Bi)^2)\n# Inner product\t'ip'\td = 1.0 - sum(Ai * Bi)\n# Cosine similarity\t'cosine'\td = 1.0 - sum(Ai * Bi) / sqrt(sum(Ai * Ai) * sum(Bi * Bi))\n# Note that inner product is not an actual metric. An element can be closer to some other element than to itself. That allows some speedup if you remove all elements that are not the closest to themselves from the index\n# 一般不用inner product 也就是ip ","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n#retriever = db.as_retriever()\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#db","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n    就回答不知道，不要编造答案.\n\n    {context}\n\n    {history}\n    问: {question}\n    答:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\nmemory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        print(res)\n        print(\"*******\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS INFO---------------------------\")\n            print(f\"score_threshold={score_threshold}\")\n            for document in docs:\n                print(\"\\n> [来源文档]: \" + document.metadata[\"source\"] )\n                print(\"> [cosine相似度得分 (0-1之间越高越相似)]:\" + str(1.0-document.metadata[\"distances\"]) )\n                print(\">[文档片段]:\" + document.page_content)\n            if len(docs)==0:\n                print(\"没有从知识库中搜索到关联信息 上面的答案answer需要重新组织 很可能是不准确的\")\n            #print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 20230901 最新","metadata":{}},{"cell_type":"markdown","source":"# 拉代码","metadata":{}},{"cell_type":"code","source":"#rm -rf /kaggle/working/*\n%cd /kaggle/working\n!git clone -b localGPT_0831_langchain_v02  https://github.com/valkryhx/localGPT","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:41:46.309741Z","iopub.execute_input":"2023-09-05T11:41:46.310095Z","iopub.status.idle":"2023-09-05T11:41:47.338723Z","shell.execute_reply.started":"2023-09-05T11:41:46.310064Z","shell.execute_reply":"2023-09-05T11:41:47.337242Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'localGPT' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 检查分支","metadata":{}},{"cell_type":"code","source":"%cd localGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:41:53.101282Z","iopub.execute_input":"2023-09-05T11:41:53.102278Z","iopub.status.idle":"2023-09-05T11:41:54.045063Z","shell.execute_reply.started":"2023-09-05T11:41:53.102241Z","shell.execute_reply":"2023-09-05T11:41:54.043835Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[Errno 2] No such file or directory: 'localGPT'\n/kaggle/working/localGPT\nOn branch localGPT_0831_langchain_v02\nYour branch is up to date with 'origin/localGPT_0831_langchain_v02'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\t\u001b[31mmodified:   localGPTUI/localGPTUI.py\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.rtl.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.rtl.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.rtl.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.rtl.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.rtl.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.rtl.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.rtl.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.rtl.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.rtl.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.rtl.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.rtl.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.rtl.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.rtl.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.rtl.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.rtl.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.rtl.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.bundle.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.bundle.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.bundle.min.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.bundle.min.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.esm.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.esm.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.esm.min.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.esm.min.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/jquery-3.2.1.min.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/jquery/3.6.0/jquery.min.js\u001b[m\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t\u001b[31mDB/\u001b[m\n\t\u001b[31m__pycache__/\u001b[m\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:43:14.176529Z","iopub.execute_input":"2023-09-05T11:43:14.176947Z","iopub.status.idle":"2023-09-05T11:45:01.349846Z","shell.execute_reply.started":"2023-09-05T11:43:14.176912Z","shell.execute_reply":"2023-09-05T11:45:01.348702Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 12, done.\u001b[K\nremote: Counting objects: 100% (12/12), done.\u001b[K\nremote: Compressing objects: 100% (6/6), done.\u001b[K\nremote: Total 12 (delta 6), reused 12 (delta 6), pack-reused 0\u001b[K\nUnpacking objects: 100% (12/12), 14.23 KiB | 1.78 MiB/s, done.\nFrom https://github.com/valkryhx/localGPT\n   1c0441f..2a65420  localGPT_0831_langchain_v02 -> origin/localGPT_0831_langchain_v02\n   4964c50..0b73a01  main       -> origin/main\nUpdating 1c0441f..2a65420\nFast-forward\n localgpt-success.ipynb       |  1 \u001b[32m+\u001b[m\n my_chatglm_llm.py            |  4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n my_chatglm_llm_from_qlora.py | 14 \u001b[32m++++++++++\u001b[m\u001b[31m----\u001b[m\n 3 files changed, 13 insertions(+), 6 deletions(-)\n create mode 100644 localgpt-success.ipynb\nIgnoring protobuf: markers 'sys_platform == \"darwin\" and platform_machine != \"arm64\"' don't match your environment\nIgnoring protobuf: markers 'sys_platform == \"darwin\" and platform_machine == \"arm64\"' don't match your environment\nIgnoring bitsandbytes-windows: markers 'sys_platform == \"win32\"' don't match your environment\nCollecting langchain==0.0.277 (from -r requirements.txt (line 2))\n  Downloading langchain-0.0.277-py3-none-any.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting chromadb==0.4.6 (from -r requirements.txt (line 3))\n  Downloading chromadb-0.4.6-py3-none-any.whl (405 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20221105 (from -r requirements.txt (line 5))\n  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting InstructorEmbedding (from -r requirements.txt (line 6))\n  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\nCollecting sentence-transformers (from -r requirements.txt (line 7))\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting faiss-cpu (from -r requirements.txt (line 8))\n  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.16.4)\nRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (4.30.2)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 11))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting protobuf==3.20.0 (from -r requirements.txt (line 12))\n  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting docx2txt (from -r requirements.txt (line 16))\n  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting unstructured (from -r requirements.txt (line 17))\n  Downloading unstructured-0.10.12-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting urllib3==1.26.6 (from -r requirements.txt (line 20))\n  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate==0.21.0 (from -r requirements.txt (line 21))\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting bitsandbytes==0.41.1 (from -r requirements.txt (line 22))\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (8.1.3)\nRequirement already satisfied: flask in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (2.3.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (2.31.0)\nCollecting streamlit (from -r requirements.txt (line 29))\n  Downloading streamlit-1.26.0-py2.py3-none-any.whl (8.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting Streamlit-extras (from -r requirements.txt (line 30))\n  Downloading streamlit_extras-0.3.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (3.1.2)\nCollecting loguru (from -r requirements.txt (line 34))\n  Downloading loguru-0.7.1-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (3.8.4)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (0.5.9)\nCollecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.277->-r requirements.txt (line 2))\n  Downloading langsmith-0.0.33-py3-none-any.whl (36 kB)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (2.8.4)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (1.23.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (1.10.10)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (8.2.2)\nCollecting chroma-hnswlib==0.7.2 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (0.98.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (0.22.0)\nCollecting posthog>=2.4.0 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (4.6.3)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (0.13.3)\nCollecting pypika>=0.48.9 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (4.65.0)\nCollecting overrides>=7.3.1 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (5.12.0)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105->-r requirements.txt (line 5)) (3.1.0)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105->-r requirements.txt (line 5)) (41.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (3.12.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (2023.6.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (0.3.1)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 11))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 11)) (1.11.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 11)) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 11)) (9.0.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 11)) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (1.11.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (0.1.99)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 9)) (2023.6.0)\nCollecting chardet (from unstructured->-r requirements.txt (line 17))\n  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting filetype (from unstructured->-r requirements.txt (line 17))\n  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nCollecting python-magic (from unstructured->-r requirements.txt (line 17))\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 17)) (4.9.3)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 17)) (0.9.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 17)) (4.12.2)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 17)) (2.6.0)\nRequirement already satisfied: Werkzeug>=2.3.3 in /opt/conda/lib/python3.10/site-packages (from flask->-r requirements.txt (line 25)) (2.3.6)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->-r requirements.txt (line 25)) (3.1.2)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->-r requirements.txt (line 25)) (2.1.2)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask->-r requirements.txt (line 25)) (1.6.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 26)) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 26)) (2023.5.7)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (5.0.1)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (4.2.4)\nRequirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (6.7.0)\nRequirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (1.5.3)\nRequirement already satisfied: pillow<10,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (9.5.0)\nRequirement already satisfied: pyarrow>=6.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (11.0.0)\nRequirement already satisfied: pympler<2,>=0.9 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (1.0.1)\nRequirement already satisfied: python-dateutil<3,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (2.8.2)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (13.4.2)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (0.10.2)\nCollecting tzlocal<5,>=1.1 (from streamlit->-r requirements.txt (line 29))\n  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\nCollecting validators<1,>=0.2 (from streamlit->-r requirements.txt (line 29))\n  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (3.1.31)\nCollecting pydeck<1,>=0.8 (from streamlit->-r requirements.txt (line 29))\n  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 29)) (6.3.2)\nCollecting watchdog>=2.1.5 (from streamlit->-r requirements.txt (line 29))\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting htbuilder==0.6.1 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading htbuilder-0.6.1.tar.gz (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting markdownlit>=0.0.5 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading markdownlit-0.0.7-py3-none-any.whl (15 kB)\nCollecting st-annotated-text>=3.0.0 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading st-annotated-text-4.0.0.tar.gz (7.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting streamlit-camera-input-live>=0.2.0 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading streamlit_camera_input_live-0.2.0-py3-none-any.whl (6.6 kB)\nCollecting streamlit-card>=0.0.4 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading streamlit_card-0.0.61-py3-none-any.whl (680 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.5/680.5 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting streamlit-embedcode>=0.1.2 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading streamlit_embedcode-0.1.2-py3-none-any.whl (3.5 kB)\nCollecting streamlit-faker>=0.0.2 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading streamlit_faker-0.0.2-py3-none-any.whl (9.8 kB)\nCollecting streamlit-image-coordinates<0.2.0,>=0.1.1 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading streamlit_image_coordinates-0.1.6-py3-none-any.whl (6.3 kB)\nCollecting streamlit-keyup>=0.1.9 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading streamlit_keyup-0.2.0-py3-none-any.whl (7.4 kB)\nCollecting streamlit-toggle-switch>=1.0.2 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading streamlit_toggle_switch-1.0.2-py3-none-any.whl (635 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.4/635.4 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting streamlit-vertical-slider>=1.0.2 (from Streamlit-extras->-r requirements.txt (line 30))\n  Downloading streamlit_vertical_slider-1.0.2-py3-none-any.whl (624 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from htbuilder==0.6.1->Streamlit-extras->-r requirements.txt (line 30)) (9.1.0)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl->-r requirements.txt (line 33)) (1.1.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (1.3.1)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 29)) (4.17.3)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 29)) (0.12.0)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->-r requirements.txt (line 5)) (1.15.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.277->-r requirements.txt (line 2)) (3.19.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.277->-r requirements.txt (line 2)) (1.5.1)\nRequirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.277->-r requirements.txt (line 2)) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r requirements.txt (line 3)) (0.27.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 29)) (4.0.10)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit->-r requirements.txt (line 29)) (3.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 25)) (2.1.3)\nRequirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from markdownlit>=0.0.5->Streamlit-extras->-r requirements.txt (line 30)) (3.4.3)\nCollecting favicon (from markdownlit>=0.0.5->Streamlit-extras->-r requirements.txt (line 30))\n  Downloading favicon-0.7.0-py2.py3-none-any.whl (5.9 kB)\nCollecting pymdown-extensions (from markdownlit>=0.0.5->Streamlit-extras->-r requirements.txt (line 30))\n  Downloading pymdown_extensions-10.3-py3-none-any.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3)) (23.5.26)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3)) (1.12)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.2->-r requirements.txt (line 10)) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit->-r requirements.txt (line 29)) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.6->-r requirements.txt (line 3)) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.6->-r requirements.txt (line 3)) (2.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 29)) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 29)) (2.15.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.277->-r requirements.txt (line 2)) (2.0.2)\nCollecting faker (from streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 30))\n  Downloading Faker-19.3.1-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 30)) (3.7.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5->-r requirements.txt (line 11)) (3.1)\nCollecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit->-r requirements.txt (line 29))\n  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (0.19.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (11.0.3)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->unstructured->-r requirements.txt (line 17)) (2.3.2.post1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 7)) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 7)) (3.1.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->-r requirements.txt (line 5)) (2.21)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 29)) (5.0.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 29)) (0.19.3)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r requirements.txt (line 29)) (0.1.0)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r requirements.txt (line 3)) (3.7.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.277->-r requirements.txt (line 2)) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 30)) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 30)) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 30)) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 30)) (1.4.4)\nRequirement already satisfied: tzdata in /opt/conda/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit->-r requirements.txt (line 29)) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3)) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r requirements.txt (line 3)) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r requirements.txt (line 3)) (1.1.1)\nBuilding wheels for collected packages: deepspeed, chroma-hnswlib, sentence-transformers, docx2txt, htbuilder, pypika, st-annotated-text\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844547 sha256=4c982594a0f3fe279dde860407aa3430916d7cf2b8391c4962719aa09efa3670\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\n  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=200798 sha256=3f308be75d1f8c3a6bf6b0710c06c01049729e2449c7be95d53235b18b412ecb\n  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=7b25e049429e657e47e2577170389225a2e47fdb802876653df5423dcc438812\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3978 sha256=16b0118bb0beff1321317b840173623a2d7201c2ace33a801cd22120d9a747c1\n  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n  Building wheel for htbuilder (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for htbuilder: filename=htbuilder-0.6.1-py3-none-any.whl size=12456 sha256=02e883e27575cc936055c11f7fd6cc03cd456ecb401f6a7d1161ffad342e66a4\n  Stored in directory: /root/.cache/pip/wheels/12/d3/e9/e499c6b18281f756e41e385735cf3ea6980baf126f093b0ba1\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=bd1462406718c17647806d5a27a08d6692fd16f72c759c638f6d27cd338d2346\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n  Building wheel for st-annotated-text (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for st-annotated-text: filename=st_annotated_text-4.0.0-py3-none-any.whl size=8923 sha256=bf665480350e8f2b0318f2d9aff1119e648d7468281b972d7b2fdbfca10cebd4\n  Stored in directory: /root/.cache/pip/wheels/75/24/e8/1c0562e6308f0fff00c6f25972c0bf0a78fbf2be2dc65ef148\nSuccessfully built deepspeed chroma-hnswlib sentence-transformers docx2txt htbuilder pypika st-annotated-text\nInstalling collected packages: pypika, monotonic, InstructorEmbedding, hjson, filetype, faiss-cpu, docx2txt, bitsandbytes, watchdog, validators, urllib3, pytz-deprecation-shim, python-magic, pymdown-extensions, pulsar-client, protobuf, overrides, loguru, humanfriendly, htbuilder, chroma-hnswlib, chardet, tzlocal, st-annotated-text, pydeck, faker, coloredlogs, posthog, pdfminer.six, onnxruntime, langsmith, favicon, deepspeed, accelerate, streamlit, chromadb, unstructured, streamlit-vertical-slider, streamlit-toggle-switch, streamlit-keyup, streamlit-image-coordinates, streamlit-embedcode, streamlit-card, streamlit-camera-input-live, sentence-transformers, langchain, streamlit-faker, markdownlit, Streamlit-extras\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.15\n    Uninstalling urllib3-1.26.15:\n      Successfully uninstalled urllib3-1.26.15\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: overrides\n    Found existing installation: overrides 6.5.0\n    Uninstalling overrides-6.5.0:\n      Successfully uninstalled overrides-6.5.0\n  Attempting uninstall: tzlocal\n    Found existing installation: tzlocal 5.0.1\n    Uninstalling tzlocal-5.0.1:\n      Successfully uninstalled tzlocal-5.0.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.0 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndistributed 2023.3.2.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ngoogle-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-artifact-registry 1.8.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-datastore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-dlp 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-language 2.10.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-monitoring 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-pubsub 2.17.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\ngoogle-cloud-pubsub 2.17.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\ngoogle-cloud-resource-manager 1.10.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-spanner 3.36.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-translate 3.11.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-videointelligence 2.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngrpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.14.0 requires protobuf>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nsentry-sdk 1.27.1 requires urllib3>=1.26.11; python_version >= \"3.6\", but you have urllib3 1.26.6 which is incompatible.\ntensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\ntensorflow-serving-api 2.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed InstructorEmbedding-1.0.1 Streamlit-extras-0.3.0 accelerate-0.21.0 bitsandbytes-0.41.1 chardet-5.2.0 chroma-hnswlib-0.7.2 chromadb-0.4.6 coloredlogs-15.0.1 deepspeed-0.9.5 docx2txt-0.8 faiss-cpu-1.7.4 faker-19.3.1 favicon-0.7.0 filetype-1.2.0 hjson-3.1.0 htbuilder-0.6.1 humanfriendly-10.0 langchain-0.0.277 langsmith-0.0.33 loguru-0.7.1 markdownlit-0.0.7 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 pdfminer.six-20221105 posthog-3.0.2 protobuf-3.20.0 pulsar-client-3.3.0 pydeck-0.8.0 pymdown-extensions-10.3 pypika-0.48.9 python-magic-0.4.27 pytz-deprecation-shim-0.1.0.post0 sentence-transformers-2.2.2 st-annotated-text-4.0.0 streamlit-1.26.0 streamlit-camera-input-live-0.2.0 streamlit-card-0.0.61 streamlit-embedcode-0.1.2 streamlit-faker-0.0.2 streamlit-image-coordinates-0.1.6 streamlit-keyup-0.2.0 streamlit-toggle-switch-1.0.2 streamlit-vertical-slider-1.0.2 tzlocal-4.3.1 unstructured-0.10.12 urllib3-1.26.16 validators-0.22.0 watchdog-3.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 事先构建一个Chroma db  目前已经支持在SOURCE_DOCUMENTS 目录下新建嵌套目录和文件\n# https://github.com/valkryhx/localGPT/blob/localGPT_0831_langchain_v02/ingest.py#L131\n# ingest的 131和132行分别按照不同粒度大小切分\n","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:46:20.147463Z","iopub.execute_input":"2023-09-05T11:46:20.147845Z","iopub.status.idle":"2023-09-05T11:47:27.001172Z","shell.execute_reply.started":"2023-09-05T11:46:20.147812Z","shell.execute_reply":"2023-09-05T11:47:27.000035Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\n\u001b[32m2023-09-05 11:46:28.222\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mBegin to  split\u001b[0m\n\u001b[32m2023-09-05 11:46:31.362\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m139\u001b[0m - \u001b[31m\u001b[1mLoaded 5 documents from /kaggle/working/localGPT/SOURCE_DOCUMENTS\u001b[0m\n\u001b[32m2023-09-05 11:46:31.363\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m140\u001b[0m - \u001b[31m\u001b[1mSplit into 1327 chunks of text\u001b[0m\n\u001b[32m2023-09-05 11:46:31.363\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m143\u001b[0m - \u001b[31m\u001b[1mEMBEDDING_MODEL_NAME=moka-ai/m3e-base\u001b[0m\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nDownloading (…)3c64c/.gitattributes: 100%|█| 1.53k/1.53k [00:00<00:00, 8.72MB/s]\nDownloading (…)_Pooling/config.json: 100%|█████| 190/190 [00:00<00:00, 1.28MB/s]\nDownloading (…)51cbe3c64c/README.md: 100%|█| 26.6k/26.6k [00:00<00:00, 10.8MB/s]\nDownloading (…)cbe3c64c/config.json: 100%|█████| 932/932 [00:00<00:00, 4.37MB/s]\nDownloading model.safetensors: 100%|██████████| 409M/409M [00:02<00:00, 195MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 409M/409M [00:01<00:00, 215MB/s]\nDownloading (…)nce_bert_config.json: 100%|████| 53.0/53.0 [00:00<00:00, 299kB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████| 125/125 [00:00<00:00, 689kB/s]\nDownloading (…)3c64c/tokenizer.json: 100%|███| 439k/439k [00:00<00:00, 5.33MB/s]\nDownloading (…)okenizer_config.json: 100%|█████| 342/342 [00:00<00:00, 2.81MB/s]\nDownloading (…)51cbe3c64c/vocab.txt: 100%|███| 110k/110k [00:00<00:00, 35.3MB/s]\nDownloading (…)be3c64c/modules.json: 100%|█████| 229/229 [00:00<00:00, 1.82MB/s]\nload INSTRUCTOR_Transformer\n[2023-09-05 11:46:42,331] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 查看或者删除DB  方便更新文件后重建\n","metadata":{}},{"cell_type":"code","source":"# !ls ./DB\n# !rm -rf DB","metadata":{"execution":{"iopub.status.busy":"2023-09-01T06:14:13.327171Z","iopub.execute_input":"2023-09-01T06:14:13.328435Z","iopub.status.idle":"2023-09-01T06:14:15.594703Z","shell.execute_reply.started":"2023-09-01T06:14:13.328392Z","shell.execute_reply":"2023-09-01T06:14:15.593334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import  注意MyChroma.py 的 MyChroma class也要 import\n# 由于MyChroma中有from __future__ import annotations（不能随便删）   from __future__ 这种import必须放在第一句","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n# codes are just from local_GPT.py\n\nimport logging\n\nimport click\nimport torch\n#from auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory ,ConversationBufferWindowMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n#from langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n  # 注意这一句 是import自己自定义的MyChroma\nfrom MyChroma import MyChroma\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:47:48.197102Z","iopub.execute_input":"2023-09-05T11:47:48.198149Z","iopub.status.idle":"2023-09-05T11:48:00.447521Z","shell.execute_reply.started":"2023-09-05T11:47:48.198093Z","shell.execute_reply":"2023-09-05T11:48:00.446522Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[2023-09-05 11:47:54,977] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 加载词向量模型 \n# 加载Chroma向量数据库\n# 设置db的相似度metric为cosine distance\n# 设置retriever的search类型为带threshold的 设置最多返回的k（结果集大小）为3","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n#retriever = db.as_retriever()\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:48:04.863259Z","iopub.execute_input":"2023-09-05T11:48:04.863651Z","iopub.status.idle":"2023-09-05T11:48:07.554304Z","shell.execute_reply.started":"2023-09-05T11:48:04.863619Z","shell.execute_reply":"2023-09-05T11:48:07.553193Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-09-05 11:48:06.072\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[31m\u001b[1mEMBEDDING_MODEL_NAME=moka-ai/m3e-base\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 加载llm chatglm2-6b","metadata":{}},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:48:12.268606Z","iopub.execute_input":"2023-09-05T11:48:12.268988Z","iopub.status.idle":"2023-09-05T11:50:51.587352Z","shell.execute_reply.started":"2023-09-05T11:48:12.268953Z","shell.execute_reply":"2023-09-05T11:50:51.586436Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827230781c0d41f6a5b4e03105c0cc8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8d5eadfbb0b4118812fb620cb260e65"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3323b29774bf406cac5224e05c407665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d00083af2e4175ae7e17bf7ae109c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b73757275bb4279b89b8a6c5f602ea9"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a15b0d34ec6844a0a8e8cd83427afcb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db7dacd8f28b41aba25dab9dad1e4da1"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a220bbcf16d94bb39b9e52baf0215e3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ab28021ddac43d19c5753eed797ef79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"789568ace6dc4adebe8b29755ebc81d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87ec5496ec142788178094606a7a629"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80e0d8c14f8045009ff90b886a87eec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bf796b493ed44acbfab3034fd0742e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ef9d8ac28347288604a8798d873300"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a82e0d2a84c54ee99a1f7f6434fe4172"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ce5e8131de4cceade722b4e6fc6636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"110507a70d34432d89ba47a43c29ce3b"}},"metadata":{}}]},{"cell_type":"markdown","source":"# 定义prompt template 主要是选择上history、context、question字段拼接\n#  <font color=red>这个memory很重要 目前我们使用窗口为2 也就是最多存两条memroy的ConversationBufferWindowMemory\n    参考其他的memroy https://zhuanlan.zhihu.com/p/646852594qa(query)\n    另外要注意的是\n    template = \"\"\"现提供如下信息:\n    history={memory_history}\n    context={context}\n    请使用上述信息回答:{question}\"\"\"\n\n    这个模板中 {context} 这个str名不能改 这是  StuffDocumentsChain的llm_chain input_variables这个list 中必须包含的 \n    因为从db搜索回的结果一定要传给这个固定名字的变量。PromptTemplate(input_variables=[ \"context\"...])中也一定要有这个str 'context'\n\n    这个模板中的{history}其实是memory的memory_key 字段 所以可以把{history}换成任意的str 比如{memory_history} 我已经换了  PromptTemplate(input_variables 也要换成对应的\n    这个模板中的{question}是memory的input_key,根据memory的input_key 也可以随便换 PromptTemplate(input_variables也要对应换成一致即可 这个memory的input_key 必须要有 不然后面代码中的qa(query) 不能正常查询db\n</font>\n<font color=red>也可以在template中不使用{memory_history} \n 这个字段的值不是chatglm的model.chat产生的那个history 而是langchain的memory 是整齐的由memory保存的只带有question 和 answer 格式友好的历史对话 例如 k = 2时 ，memory_history为\n\nHuman:飞机票报销\nAI: 1. 国内LTC项目管理系统的负责人是谁？ 答：请联系雷彪 (blei@fiberhome.com)\n2. 谁负责管理国内LTC项目管理系统？ 答：请联系雷彪 (blei@fiberhome.com)\n3. 国内LTC项目管理系统的创始人是谁？ 答：请联系雷彪 (blei@fiberhome.com)\n4. 谁负责维护国内LTC项目管理系统？ 答：请联系雷彪 (blei@fiberhome.com)\n5. 国内LTC项目管理系统的拥有者是谁？ 答：请联系雷彪 (blei@fiberhome.com)\n\nHuman: 驻外怎么报销\nAI: 驻外人员日常费用报销的流程是什么？\", \"input\": \"\", \"output\": \"1.提交费用报销申请单，选择\"驻外\"费用类别，并填写相关信息。\\n2.选择要报销的发票，并提交。\\n3.负责人审批后，生成报销单，并交还给驻外人员。\\n4.驻外人员携带发票和报销单回公司办理报销手续。  \n    \n</font>\n# 如果不使用memroy维护的格式友好的history 那就在template中把history={memory_history} 删除\n# 但是我觉得由chatglm.chat产生的history包含了搜索信息 很杂乱 最好不用那个杂乱的history ，宁可在template里面写上history={memory_history\n\n# 我修改了https://github.com/valkryhx/localGPT/blob/localGPT_0831_langchain_v02/my_chatglm_llm.py#L74\n# 这个my_chatglm_llm 的产生history的方式 目前是根据template格式切出来最后一个提问和回答 不带乱七八糟的search doc 上下文\n# 定义RetrievalQA.from_chain_type","metadata":{}},{"cell_type":"markdown","source":"#  memory = ConversationBufferWindowMemory(k=2 ...) 只记住最近的2条历史对话","metadata":{}},{"cell_type":"code","source":"# template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n#     就回答不知道，不要编造答案.\n#     history={history}\n#     context={context}\n\n#     问: {question}\n#     答:\"\"\"\n\n\ntemplate = \"\"\"现提供如下信息:\n    history={memory_history}\n    context={context}\n    请使用上述信息回答:{question}\"\"\"\n\nprompt = PromptTemplate(input_variables=[ \"question\",'memory_history',\"context\",], template=template)\n#memory = ConversationBufferMemory(input_key=\"question\", memory_key='history')\n# https://zhuanlan.zhihu.com/p/646852594\n#一开始用这个memory = ConversationBufferWindowMemory(k=2,input_key=\"question\", memory_key='memory_history')\n\n# 参考 https://python.langchain.com/docs/modules/memory/types/buffer_window 加了return_message=True\nmemory = ConversationBufferWindowMemory(k=3,input_key=\"question\", memory_key='memory_history',return_messages=True)\n\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:53:46.120247Z","iopub.execute_input":"2023-09-05T11:53:46.120641Z","iopub.status.idle":"2023-09-05T11:53:46.131953Z","shell.execute_reply.started":"2023-09-05T11:53:46.120610Z","shell.execute_reply":"2023-09-05T11:53:46.130931Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[32m2023-09-05 11:53:46.125\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[31m\u001b[1mFalse\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 运行","metadata":{}},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n       \n        print(\"res=\",res)\n        my_memory = memory.load_memory_variables({})\n        \n        # memory参数为4 但是这儿可以强行只保留最后2段qa\n        retain = 2\n        retain *=2 # 必须的\n        my_memory[\"memory_history\"]=my_memory[\"memory_history\"][-retain:]\n\n        print(f\"\\nmy_memory={my_memory}\")\n        print(f\"len_q_a_of_memory={len(my_memory['memory_history'])}\")\n        print(\"*******\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS INFO---------------------------\")\n            print(f\"score_threshold={score_threshold}\")\n            for document in docs:\n                print(\"\\n> [来源文档]: \" + document.metadata[\"source\"] )\n                print(\"> [cosine相似度得分 (0-1之间越高越相似)]:\" + str(1.0-document.metadata[\"distances\"]) )\n                print(\">[文档片段]:\" + document.page_content)\n            if len(docs)==0:\n                print(\"没有从知识库中搜索到关联信息 上面的答案answer需要重新组织 很可能是不准确的\")\n            #print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:28:25.154116Z","iopub.execute_input":"2023-09-05T12:28:25.154507Z","iopub.status.idle":"2023-09-05T12:30:58.603238Z","shell.execute_reply.started":"2023-09-05T12:28:25.154476Z","shell.execute_reply":"2023-09-05T12:30:58.602170Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdin","text":"输入问题:\n 怎么做蛋糕\n"},{"name":"stderr","text":"\u001b[32m2023-09-05 12:28:33.933\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mprompt=现提供如下信息:\n    history=[HumanMessage(content='所以孙悟空是？', additional_kwargs={}, example=False), AIMessage(content='孙悟空是中国神话故事《西游记》中的重要角色，他是唐僧的大徒弟，拥有强大的法力，能够施展七十二变。根据您提供的信息，孙悟空可能是唐僧的徒弟，拥有报销流程的相关信息。但是，由于您提供的信息比较简短，无法确定孙悟空是否真的是唐僧的徒弟，也无法确定报销流程的具体细节。', additional_kwargs={}, example=False), HumanMessage(content='怎么制作蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False)]\n    context=hereby prohibited.\n\nhereby prohibited.\n\nexclusive Right to their respective Writings and Discoveries;\n    请使用上述信息回答:怎么做蛋糕\u001b[0m\n\u001b[32m2023-09-05 12:28:41.027\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m67\u001b[0m - \u001b[31m\u001b[1mupdated_history[-1]完整部分包括memory_history,context和question=(\"现提供如下信息:\\n    history=[HumanMessage(content='所以孙悟空是？', additional_kwargs={}, example=False), AIMessage(content='孙悟空是中国神话故事《西游记》中的重要角色，他是唐僧的大徒弟，拥有强大的法力，能够施展七十二变。根据您提供的信息，孙悟空可能是唐僧的徒弟，拥有报销流程的相关信息。但是，由于您提供的信息比较简短，无法确定孙悟空是否真的是唐僧的徒弟，也无法确定报销流程的具体细节。', additional_kwargs={}, example=False), HumanMessage(content='怎么制作蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\\\n\\\\n1. 购买蛋糕配方并准备材料\\\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\\\n3. 加入鸡蛋，搅拌均匀\\\\n4. 将面糊倒入准备好的蛋糕模具中\\\\n5. 放入烤箱中，按照配方要求烤制\\\\n6. 取出烤好的蛋糕，冷却后即可享用\\\\n\\\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False)]\\n    context=hereby prohibited.\\n\\nhereby prohibited.\\n\\nexclusive Right to their respective Writings and Discoveries;\\n    请使用上述信息回答:怎么做蛋糕\", '根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。')\u001b[0m\n\u001b[32m2023-09-05 12:28:41.029\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m77\u001b[0m - \u001b[31m\u001b[1mhistory[-1]仅仅包含最后的question部分,history=[('怎么做蛋糕', '根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。')]\nlen(self.history)=1\u001b[0m\n\u001b[32m2023-09-05 12:28:41.030\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m78\u001b[0m - \u001b[31m\u001b[1mchar_len_total =188\u001b[0m\n\u001b[32m2023-09-05 12:28:41.032\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m80\u001b[0m - \u001b[34m\u001b[1mitem in history=('怎么做蛋糕', '根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。')\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"res= {'query': '怎么做蛋糕', 'result': '根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', 'source_documents': [Document(page_content='hereby prohibited.', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.20948520095233025}), Document(page_content='hereby prohibited.', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.20948576927185059}), Document(page_content='exclusive Right to their respective Writings and Discoveries;', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.21377956867218018})]}\n\nmy_memory={'memory_history': [HumanMessage(content='怎么制作蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False), HumanMessage(content='怎么做蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False)]}\nlen_q_a_of_memory=4\n*******\n\n\n> Question:\n怎么做蛋糕\n\n> Answer:\n根据提供的信息，要制作蛋糕，可以选择以下步骤：\n\n1. 购买蛋糕配方并准备材料\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\n3. 加入鸡蛋，搅拌均匀\n4. 将面糊倒入准备好的蛋糕模具中\n5. 放入烤箱中，按照配方要求烤制\n6. 取出烤好的蛋糕，冷却后即可享用\n\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。\n----------------------------------SOURCE DOCUMENTS INFO---------------------------\nscore_threshold=0.6\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7905147990476697\n>[文档片段]:hereby prohibited.\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7905142307281494\n>[文档片段]:hereby prohibited.\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7862204313278198\n>[文档片段]:exclusive Right to their respective Writings and Discoveries;\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"输入问题:\n 姑妈是？\n"},{"name":"stderr","text":"\u001b[32m2023-09-05 12:29:22.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mprompt=现提供如下信息:\n    history=[HumanMessage(content='怎么制作蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False), HumanMessage(content='怎么做蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False)]\n    context=hereby prohibited.\n\nhereby prohibited.\n\nunder their Authority; - to all Cases affecting Ambassa- \ndors, other public Ministers and Consuls; - to all Cases of \nadmiralty and maritime Jurisdiction; - to Controversies to\n    请使用上述信息回答:姑妈是？\u001b[0m\n\u001b[32m2023-09-05 12:29:24.498\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m67\u001b[0m - \u001b[31m\u001b[1mupdated_history[-1]完整部分包括memory_history,context和question=(\"现提供如下信息:\\n    history=[HumanMessage(content='怎么制作蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\\\n\\\\n1. 购买蛋糕配方并准备材料\\\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\\\n3. 加入鸡蛋，搅拌均匀\\\\n4. 将面糊倒入准备好的蛋糕模具中\\\\n5. 放入烤箱中，按照配方要求烤制\\\\n6. 取出烤好的蛋糕，冷却后即可享用\\\\n\\\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False), HumanMessage(content='怎么做蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\\\n\\\\n1. 购买蛋糕配方并准备材料\\\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\\\n3. 加入鸡蛋，搅拌均匀\\\\n4. 将面糊倒入准备好的蛋糕模具中\\\\n5. 放入烤箱中，按照配方要求烤制\\\\n6. 取出烤好的蛋糕，冷却后即可享用\\\\n\\\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False)]\\n    context=hereby prohibited.\\n\\nhereby prohibited.\\n\\nunder their Authority; - to all Cases affecting Ambassa- \\ndors, other public Ministers and Consuls; - to all Cases of \\nadmiralty and maritime Jurisdiction; - to Controversies to\\n    请使用上述信息回答:姑妈是？\", '根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。')\u001b[0m\n\u001b[32m2023-09-05 12:29:24.499\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m77\u001b[0m - \u001b[31m\u001b[1mhistory[-1]仅仅包含最后的question部分,history=[('姑妈是？', '根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。')]\nlen(self.history)=1\u001b[0m\n\u001b[32m2023-09-05 12:29:24.501\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m78\u001b[0m - \u001b[31m\u001b[1mchar_len_total =53\u001b[0m\n\u001b[32m2023-09-05 12:29:24.502\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m80\u001b[0m - \u001b[34m\u001b[1mitem in history=('姑妈是？', '根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。')\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"res= {'query': '姑妈是？', 'result': '根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。', 'source_documents': [Document(page_content='hereby prohibited.', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.22979333176561323}), Document(page_content='hereby prohibited.', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.22979384660720825}), Document(page_content='under their Authority; - to all Cases affecting Ambassa- \\ndors, other public Ministers and Consuls; - to all Cases of \\nadmiralty and maritime Jurisdiction; - to Controversies to', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.23427033424377441})]}\n\nmy_memory={'memory_history': [HumanMessage(content='怎么做蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False), HumanMessage(content='姑妈是？', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。', additional_kwargs={}, example=False)]}\nlen_q_a_of_memory=4\n*******\n\n\n> Question:\n姑妈是？\n\n> Answer:\n根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。\n----------------------------------SOURCE DOCUMENTS INFO---------------------------\nscore_threshold=0.6\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7702066682343868\n>[文档片段]:hereby prohibited.\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7702061533927917\n>[文档片段]:hereby prohibited.\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7657296657562256\n>[文档片段]:under their Authority; - to all Cases affecting Ambassa- \ndors, other public Ministers and Consuls; - to all Cases of \nadmiralty and maritime Jurisdiction; - to Controversies to\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"输入问题:\n 太阳多热\n"},{"name":"stderr","text":"\u001b[32m2023-09-05 12:29:41.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mprompt=现提供如下信息:\n    history=[HumanMessage(content='怎么做蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\n\\n1. 购买蛋糕配方并准备材料\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\n3. 加入鸡蛋，搅拌均匀\\n4. 将面糊倒入准备好的蛋糕模具中\\n5. 放入烤箱中，按照配方要求烤制\\n6. 取出烤好的蛋糕，冷却后即可享用\\n\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False), HumanMessage(content='姑妈是？', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。', additional_kwargs={}, example=False)]\n    context=exclusive Right to their respective Writings and Discoveries;\n\nexclusive Right to their respective Writings and Discoveries;\n\nunder this Constitution\n    请使用上述信息回答:太阳多热\u001b[0m\n\u001b[32m2023-09-05 12:29:43.057\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m67\u001b[0m - \u001b[31m\u001b[1mupdated_history[-1]完整部分包括memory_history,context和question=(\"现提供如下信息:\\n    history=[HumanMessage(content='怎么做蛋糕', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，要制作蛋糕，可以选择以下步骤：\\\\n\\\\n1. 购买蛋糕配方并准备材料\\\\n2. 将配方中的面粉、糖、泡打粉、盐等食材按照要求混合在一起\\\\n3. 加入鸡蛋，搅拌均匀\\\\n4. 将面糊倒入准备好的蛋糕模具中\\\\n5. 放入烤箱中，按照配方要求烤制\\\\n6. 取出烤好的蛋糕，冷却后即可享用\\\\n\\\\n请注意，以上步骤仅供参考，实际制作蛋糕的流程可能因个人口味、食材品质等因素而有所不同。', additional_kwargs={}, example=False), HumanMessage(content='姑妈是？', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。', additional_kwargs={}, example=False)]\\n    context=exclusive Right to their respective Writings and Discoveries;\\n\\nexclusive Right to their respective Writings and Discoveries;\\n\\nunder this Constitution\\n    请使用上述信息回答:太阳多热\", '根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。')\u001b[0m\n\u001b[32m2023-09-05 12:29:43.058\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m77\u001b[0m - \u001b[31m\u001b[1mhistory[-1]仅仅包含最后的question部分,history=[('太阳多热', '根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。')]\nlen(self.history)=1\u001b[0m\n\u001b[32m2023-09-05 12:29:43.060\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m78\u001b[0m - \u001b[31m\u001b[1mchar_len_total =47\u001b[0m\n\u001b[32m2023-09-05 12:29:43.061\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m80\u001b[0m - \u001b[34m\u001b[1mitem in history=('太阳多热', '根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。')\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"res= {'query': '太阳多热', 'result': '根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。', 'source_documents': [Document(page_content='exclusive Right to their respective Writings and Discoveries;', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.21681749820709229}), Document(page_content='exclusive Right to their respective Writings and Discoveries;', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.21681749820709229}), Document(page_content='under this Constitution', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf', 'distances': 0.22320562601089478})]}\n\nmy_memory={'memory_history': [HumanMessage(content='姑妈是？', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。', additional_kwargs={}, example=False), HumanMessage(content='太阳多热', additional_kwargs={}, example=False), AIMessage(content='根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。', additional_kwargs={}, example=False)]}\nlen_q_a_of_memory=4\n*******\n\n\n> Question:\n太阳多热\n\n> Answer:\n根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。\n----------------------------------SOURCE DOCUMENTS INFO---------------------------\nscore_threshold=0.6\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7831825017929077\n>[文档片段]:exclusive Right to their respective Writings and Discoveries;\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7831825017929077\n>[文档片段]:exclusive Right to their respective Writings and Discoveries;\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/constitution.pdf\n> [cosine相似度得分 (0-1之间越高越相似)]:0.7767943739891052\n>[文档片段]:under this Constitution\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"输入问题:\n exit\n"}]},{"cell_type":"markdown","source":"# 我们来看一看怎么从my_memory这个对象中拿到标准的history standard_history","metadata":{}},{"cell_type":"code","source":"my_memory","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:31:01.618286Z","iopub.execute_input":"2023-09-05T12:31:01.618652Z","iopub.status.idle":"2023-09-05T12:31:01.626186Z","shell.execute_reply.started":"2023-09-05T12:31:01.618622Z","shell.execute_reply":"2023-09-05T12:31:01.625206Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{'memory_history': [HumanMessage(content='姑妈是？', additional_kwargs={}, example=False),\n  AIMessage(content='根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。', additional_kwargs={}, example=False),\n  HumanMessage(content='太阳多热', additional_kwargs={}, example=False),\n  AIMessage(content='根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。', additional_kwargs={}, example=False)]}"},"metadata":{}}]},{"cell_type":"code","source":"# m = my_memory.get(\"memory_history\",None)\nlist_m = [human_msg.content for human_msg in my_memory[\"memory_history\"]]\n# 注意 step =2  每隔一个取出一对 qa 保存到standard_history\nstandard_history = [[list_m[idx],list_m[idx+1]]  for idx  in range(0,len(list_m),2)]\nprint(standard_history)    ","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:31:20.037890Z","iopub.execute_input":"2023-09-05T12:31:20.038371Z","iopub.status.idle":"2023-09-05T12:31:20.047156Z","shell.execute_reply.started":"2023-09-05T12:31:20.038330Z","shell.execute_reply":"2023-09-05T12:31:20.046190Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"[['姑妈是？', '根据提供的信息，我们无法确定姑妈的身份。请提供更多关于姑妈的信息，以便我们能够更好地回答您的问题。'], ['太阳多热', '根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 注意这是让memory强制只要最后一对qa的写法 注意是必须为一对一对的 也就是 2 4 6。。。 所以retain乘以2","metadata":{}},{"cell_type":"code","source":"retain = 1\nretain *=2 # 必须的\nmy_memory[\"memory_history\"]=my_memory[\"memory_history\"][-retain:]","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:31:55.014968Z","iopub.execute_input":"2023-09-05T12:31:55.015479Z","iopub.status.idle":"2023-09-05T12:31:55.019680Z","shell.execute_reply.started":"2023-09-05T12:31:55.015442Z","shell.execute_reply":"2023-09-05T12:31:55.018725Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"my_memory","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:31:59.338562Z","iopub.execute_input":"2023-09-05T12:31:59.338920Z","iopub.status.idle":"2023-09-05T12:31:59.345494Z","shell.execute_reply.started":"2023-09-05T12:31:59.338890Z","shell.execute_reply":"2023-09-05T12:31:59.344495Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"{'memory_history': [HumanMessage(content='太阳多热', additional_kwargs={}, example=False),\n  AIMessage(content='根据提供的信息，我们无法确定太阳有多热，因为该信息并没有提及与太阳温度相关的任何内容。', additional_kwargs={}, example=False)]}"},"metadata":{}}]}]}