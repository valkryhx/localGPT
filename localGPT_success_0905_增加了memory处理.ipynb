{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-05T11:41:36.879391Z","iopub.execute_input":"2023-09-05T11:41:36.879923Z","iopub.status.idle":"2023-09-05T11:41:36.892224Z","shell.execute_reply.started":"2023-09-05T11:41:36.879876Z","shell.execute_reply":"2023-09-05T11:41:36.891163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/localGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd localGPT/\n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat /kaggle/working/localGPT/constants.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 上面这个max_se_length 其实是https://github.com/langchain-ai/langchain/blob/0689628489967785f3a11a9f29d8f6f90930f4f4/libs/langchain/langchain/embeddings/huggingface.py#L231C9-L231C65\nBreadcrumbslangchain/libs/langchain/langchain/embeddings\n/huggingface.py 231行sentence_transformers.SentenceTransformer 加载SentenceTransformer 默认的512  要动的话需要动源码","metadata":{}},{"cell_type":"markdown","source":"# ls DB/  ingest 之后的向量文件在这里  如果不想要就 rm -rf DB 那下次就要重新ingest","metadata":{}},{"cell_type":"code","source":"#run_localGPT.py 中的query=input()无法在kaggle环境上正常执行 我们自己把代码拿出来跑\n#!python run_localGPT.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# codes are just from local_GPT.py\nimport logging\n\nimport click\nimport torch\nfrom auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h=[(\"123\"),(\"3\")]\nsum([len(item) for item in h])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity","metadata":{}},{"cell_type":"code","source":"\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = Chroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n    )\n#retriever = db.as_retriever()\n\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":0.75}\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n    就回答不知道，不要编造答案.\n\n    {context}\n\n    {history}\n    问: {question}\n    答:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\nmemory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        print(\"12345\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n            for document in docs:\n                print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n                print(document.page_content)\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Chroma和FAISS默认的相似度计算metric是L2 distance\n在Chroma.from_documents方法中加入参数\ncollection_metadata={\"hnsw:space\": \"cosine\"} 改成cosine 相似度 在[0,1]之间 越高说明两个向量越相似\n\nIs you are using Chroma, you should set the distance metric when creating a collection: https://docs.trychroma.com/usage-guide#changing-the-distance-function\n\nThe default distance is l2. That is why for me it used to give scores like 3626.016357421875 when using the function similarity_search_with_relevance_scores(). On changing it to cosine, the scores are now between (0, 1] with scores closer to 1 depicting higher similarity.\n\nChroma.from_documents(documents=documents, embedding=cohere, collection_metadata={\"hnsw:space\": \"cosine\"})\n\n参考 https://stackoverflow.com/questions/76678783/langchains-chroma-vectordb-similarity-search-with-score-and-vectordb-simil\n参考 langchain.vectorstors.Chroma 源码  https://api.python.langchain.com/en/latest/_modules/langchain/vectorstores/chroma.html\n的class Chroma类的__init__方法","metadata":{}},{"cell_type":"markdown","source":"# https://github.com/langchain-ai/langchain/issues/6481\n# https://docs.trychroma.com/usage-guide#changing-the-distance-function\n#注意这个 https://stackoverflow.com/questions/76678783/langchains-chroma-vectordb-similarity-search-with-score-and-vectordb-simil\n# https://github.com/langchain-ai/langchain/issues/5458  \n这个写了怎么在向量计算结果中加入相似度阈值和个数阈值进行过滤 结果不超过k个 并且score_threshold要大于这个0.5才参与候选\nretriever=db.as_retriever(search_type=\"similarity_score_threshold\", \n                          search_kwargs={\"k\":3, \"score_threshold\":0.5})    \nhttps://github.com/langchain-ai/langchain/blob/e60e1cdf23ad73b2e0a40034c0ddfc3c8b0c9c4d/libs/langchain/langchain/vectorstores/base.py#L460","metadata":{}},{"cell_type":"markdown","source":"# langchain使用样例\nhttps://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa","metadata":{}},{"cell_type":"markdown","source":"# python 合并字典 优雅\nhttps://segmentfault.com/a/1190000010567015","metadata":{}},{"cell_type":"code","source":"# 用新版本的https://github.com/valkryhx/localGPT   \n# branch localGPT_0831_langchain_v02","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf /kaggle/working/localGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>注意 要让Chroma使用cosine distance（注意不是cosine similarity）而非默认的L2 distance</font>  \nhttps://github.com/valkryhx/localGPT/blob/localGPT_0831_langchain_v02/ingest.py#L150 加一行   collection_metadata={\"hnsw:space\": \"cosine\"},\n在下面的# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"ip\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n也加一行\n\n# 魔改了langchain/vectorstores/chroma.py 增加了distance value输出  越相似的distance越小\n# #retriever = db.as_retriever() 改了 增加 search_type=\"similarity_score_threshold\", search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )\n # 注意 这个threshold如果取0.7 那么实际对应的是distance value小于0.3的text 也就是说distance越小 则1-distance 越大 越容易超过threshold  这符合逻辑和变量名定义  后面print时 我会用1-distance的值来显示","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b localGPT_0831_langchain_v02 https://github.com/valkryhx/localGPT  ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T14:26:24.587716Z","iopub.execute_input":"2023-09-27T14:26:24.588596Z","iopub.status.idle":"2023-09-27T14:26:25.581371Z","shell.execute_reply.started":"2023-09-27T14:26:24.588551Z","shell.execute_reply":"2023-09-27T14:26:25.580033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd localGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-09-27T14:26:36.776433Z","iopub.execute_input":"2023-09-27T14:26:36.777660Z","iopub.status.idle":"2023-09-27T14:26:37.811930Z","shell.execute_reply.started":"2023-09-27T14:26:36.777612Z","shell.execute_reply":"2023-09-27T14:26:37.810722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-27T14:26:43.390541Z","iopub.execute_input":"2023-09-27T14:26:43.390950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 使用两种不同的embedding模型可能会导致生成的向量维度不一致，报错 chromadb.errors.InvalidDimensionException: Embedding dimension 1024 does not match collection dimensionality 768\n比如先用m3e embedding模型生成的向量为768 已经存到DB目录了 再用bge-large-zhv1.5 模型生成向量长度为1024 这样就会报错 解决方法是 https://github.com/langchain-ai/langchain/issues/5046#issuecomment-1560241183 删除旧的DB 重新生成","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 重新生成   \n#ls DB\n#!rm -rf DB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# codes are just from local_GPT.py\nimport logging\n\nimport click\nimport torch\n#from auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"根据这个问题\n\nhttps://github.com/langchain-ai/langchain/issues/4710\n\nhttps://github.com/langchain-ai/langchain/issues/5416\n\n改的_results_to_docs_and_scores方法中\nmetadata={**result[1],**{\"distances\":result[2]} } or {})","metadata":{}},{"cell_type":"code","source":"\"\"\"Wrapper around ChromaDB embeddings platform.\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport uuid\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n)\n\nimport numpy as np\n\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.utils import xor_args\nfrom langchain.vectorstores.base import VectorStore\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\n\nif TYPE_CHECKING:\n    import chromadb\n    import chromadb.config\n    from chromadb.api.types import ID, OneOrMany, Where, WhereDocument\n\nlogger = logging.getLogger()\nDEFAULT_K = 4  # Number of Documents to return.\n\n\ndef _results_to_docs(results: Any) -> List[Document]:\n    return [doc for doc, _ in _results_to_docs_and_scores(results)]\n\n\ndef _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\n    return [\n        # TODO: Chroma can do batch querying,\n        # we shouldn't hard code to the 1st result\n        \n        # merge two dicts ,metadata = {**d1,**d2}\n        (Document(page_content=result[0], metadata={**result[1],**{\"distances\":result[2]} } or {}), result[2])\n        for result in zip(\n            results[\"documents\"][0],\n            results[\"metadatas\"][0],\n            results[\"distances\"][0],\n        )\n    ]\n\n\nclass MyChroma(VectorStore):\n    \"\"\"Wrapper around ChromaDB embeddings platform.\n\n    To use, you should have the ``chromadb`` python package installed.\n\n    Example:\n        .. code-block:: python\n\n                from langchain.vectorstores import Chroma\n                from langchain.embeddings.openai import OpenAIEmbeddings\n\n                embeddings = OpenAIEmbeddings()\n                vectorstore = Chroma(\"langchain_store\", embeddings)\n    \"\"\"\n\n    _LANGCHAIN_DEFAULT_COLLECTION_NAME = \"langchain\"\n\n    def __init__(\n        self,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        embedding_function: Optional[Embeddings] = None,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        collection_metadata: Optional[Dict] = None,\n        client: Optional[chromadb.Client] = None,\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\n    ) -> None:\n        \"\"\"Initialize with Chroma client.\"\"\"\n        try:\n            import chromadb\n            import chromadb.config\n        except ImportError:\n            raise ValueError(\n                \"Could not import chromadb python package. \"\n                \"Please install it with `pip install chromadb`.\"\n            )\n\n        if client is not None:\n            self._client_settings = client_settings\n            self._client = client\n            self._persist_directory = persist_directory\n        else:\n            if client_settings:\n                # If client_settings is provided with persist_directory specified,\n                # then it is \"in-memory and persisting to disk\" mode.\n                client_settings.persist_directory = (\n                    persist_directory or client_settings.persist_directory\n                )\n                if client_settings.persist_directory is not None:\n                    # Maintain backwards compatibility with chromadb < 0.4.0\n                    major, minor, _ = chromadb.__version__.split(\".\")\n                    if int(major) == 0 and int(minor) < 4:\n                        client_settings.chroma_db_impl = \"duckdb+parquet\"\n\n                _client_settings = client_settings\n            elif persist_directory:\n                # Maintain backwards compatibility with chromadb < 0.4.0\n                major, minor, _ = chromadb.__version__.split(\".\")\n                if int(major) == 0 and int(minor) < 4:\n                    _client_settings = chromadb.config.Settings(\n                        chroma_db_impl=\"duckdb+parquet\",\n                    )\n                else:\n                    _client_settings = chromadb.config.Settings(is_persistent=True)\n                _client_settings.persist_directory = persist_directory\n            else:\n                _client_settings = chromadb.config.Settings()\n            self._client_settings = _client_settings\n            self._client = chromadb.Client(_client_settings)\n            self._persist_directory = (\n                _client_settings.persist_directory or persist_directory\n            )\n\n        self._embedding_function = embedding_function\n        self._collection = self._client.get_or_create_collection(\n            name=collection_name,\n            embedding_function=self._embedding_function.embed_documents\n            if self._embedding_function is not None\n            else None,\n            metadata=collection_metadata,\n        )\n        self.override_relevance_score_fn = relevance_score_fn\n\n    @property\n    def embeddings(self) -> Optional[Embeddings]:\n        return self._embedding_function\n\n    @xor_args((\"query_texts\", \"query_embeddings\"))\n    def __query_collection(\n        self,\n        query_texts: Optional[List[str]] = None,\n        query_embeddings: Optional[List[List[float]]] = None,\n        n_results: int = 4,\n        where: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Query the chroma collection.\"\"\"\n        try:\n            import chromadb  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"Could not import chromadb python package. \"\n                \"Please install it with `pip install chromadb`.\"\n            )\n        return self._collection.query(\n            query_texts=query_texts,\n            query_embeddings=query_embeddings,\n            n_results=n_results,\n            where=where,\n            **kwargs,\n        )\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            ids (Optional[List[str]], optional): Optional list of IDs.\n\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        # TODO: Handle the case where the user doesn't provide ids on the Collection\n        if ids is None:\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n        texts = list(texts)\n        if self._embedding_function is not None:\n            embeddings = self._embedding_function.embed_documents(texts)\n        if metadatas:\n            # fill metadatas with empty dicts if somebody\n            # did not specify metadata for all texts\n            length_diff = len(texts) - len(metadatas)\n            if length_diff:\n                metadatas = metadatas + [{}] * length_diff\n            empty_ids = []\n            non_empty_ids = []\n            for idx, m in enumerate(metadatas):\n                if m:\n                    non_empty_ids.append(idx)\n                else:\n                    empty_ids.append(idx)\n            if non_empty_ids:\n                metadatas = [metadatas[idx] for idx in non_empty_ids]\n                texts_with_metadatas = [texts[idx] for idx in non_empty_ids]\n                embeddings_with_metadatas = (\n                    [embeddings[idx] for idx in non_empty_ids] if embeddings else None\n                )\n                ids_with_metadata = [ids[idx] for idx in non_empty_ids]\n                try:\n                    self._collection.upsert(\n                        metadatas=metadatas,\n                        embeddings=embeddings_with_metadatas,\n                        documents=texts_with_metadatas,\n                        ids=ids_with_metadata,\n                    )\n                except ValueError as e:\n                    if \"Expected metadata value to be\" in str(e):\n                        msg = (\n                            \"Try filtering complex metadata from the document using \"\n                            \"langchain.vectorstore.utils.filter_complex_metadata.\"\n                        )\n                        raise ValueError(e.args[0] + \"\\n\\n\" + msg)\n                    else:\n                        raise e\n            if empty_ids:\n                texts_without_metadatas = [texts[j] for j in empty_ids]\n                embeddings_without_metadatas = (\n                    [embeddings[j] for j in empty_ids] if embeddings else None\n                )\n                ids_without_metadatas = [ids[j] for j in empty_ids]\n                self._collection.upsert(\n                    embeddings=embeddings_without_metadatas,\n                    documents=texts_without_metadatas,\n                    ids=ids_without_metadatas,\n                )\n        else:\n            self._collection.upsert(\n                embeddings=embeddings,\n                documents=texts,\n                ids=ids,\n            )\n        return ids\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Run similarity search with Chroma.\n\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\n        docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\n        return [doc for doc, _ in docs_and_scores]\n\n    def similarity_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to embedding vector.\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List of Documents most similar to the query vector.\n        \"\"\"\n        results = self.__query_collection(\n            query_embeddings=embedding, n_results=k, where=filter\n        )\n        return _results_to_docs(results)\n\n    def similarity_search_by_vector_with_relevance_scores(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"\n        Return docs most similar to embedding vector and similarity score.\n\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        results = self.__query_collection(\n            query_embeddings=embedding, n_results=k, where=filter\n        )\n        return _results_to_docs_and_scores(results)\n\n    def similarity_search_with_score(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Run similarity search with Chroma with distance.\n\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        if self._embedding_function is None:\n            results = self.__query_collection(\n                query_texts=[query], n_results=k, where=filter\n            )\n        else:\n            query_embedding = self._embedding_function.embed_query(query)\n            results = self.__query_collection(\n                query_embeddings=[query_embedding], n_results=k, where=filter\n            )\n\n        return _results_to_docs_and_scores(results)\n\n    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n        \"\"\"\n        The 'correct' relevance function\n        may differ depending on a few things, including:\n        - the distance / similarity metric used by the VectorStore\n        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)\n        - embedding dimensionality\n        - etc.\n        \"\"\"\n        if self.override_relevance_score_fn:\n            return self.override_relevance_score_fn\n\n        distance = \"l2\"\n        distance_key = \"hnsw:space\"\n        metadata = self._collection.metadata\n\n        if metadata and distance_key in metadata:\n            distance = metadata[distance_key]\n\n        if distance == \"cosine\":\n            return self._cosine_relevance_score_fn\n        elif distance == \"l2\":\n            return self._euclidean_relevance_score_fn\n        elif distance == \"ip\":\n            return self._max_inner_product_relevance_score_fn\n        else:\n            raise ValueError(\n                \"No supported normalization function\"\n                f\" for distance metric of type: {distance}.\"\n                \"Consider providing relevance_score_fn to Chroma constructor.\"\n            )\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n\n        results = self.__query_collection(\n            query_embeddings=embedding,\n            n_results=fetch_k,\n            where=filter,\n            include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"],\n        )\n        mmr_selected = maximal_marginal_relevance(\n            np.array(embedding, dtype=np.float32),\n            results[\"embeddings\"][0],\n            k=k,\n            lambda_mult=lambda_mult,\n        )\n\n        candidates = _results_to_docs(results)\n\n        selected_results = [r for i, r in enumerate(candidates) if i in mmr_selected]\n        return selected_results\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            query: Text to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        if self._embedding_function is None:\n            raise ValueError(\n                \"For MMR search, you must specify an embedding function on\" \"creation.\"\n            )\n\n        embedding = self._embedding_function.embed_query(query)\n        docs = self.max_marginal_relevance_search_by_vector(\n            embedding, k, fetch_k, lambda_mult=lambda_mult, filter=filter\n        )\n        return docs\n\n    def delete_collection(self) -> None:\n        \"\"\"Delete the collection.\"\"\"\n        self._client.delete_collection(self._collection.name)\n\n    def get(\n        self,\n        ids: Optional[OneOrMany[ID]] = None,\n        where: Optional[Where] = None,\n        limit: Optional[int] = None,\n        offset: Optional[int] = None,\n        where_document: Optional[WhereDocument] = None,\n        include: Optional[List[str]] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Gets the collection.\n\n        Args:\n            ids: The ids of the embeddings to get. Optional.\n            where: A Where type dict used to filter results by.\n                   E.g. `{\"color\" : \"red\", \"price\": 4.20}`. Optional.\n            limit: The number of documents to return. Optional.\n            offset: The offset to start returning results from.\n                    Useful for paging results with limit. Optional.\n            where_document: A WhereDocument type dict used to filter by the documents.\n                            E.g. `{$contains: {\"text\": \"hello\"}}`. Optional.\n            include: A list of what to include in the results.\n                     Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n                     Ids are always included.\n                     Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n        \"\"\"\n        kwargs = {\n            \"ids\": ids,\n            \"where\": where,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"where_document\": where_document,\n        }\n\n        if include is not None:\n            kwargs[\"include\"] = include\n\n        return self._collection.get(**kwargs)\n\n    def persist(self) -> None:\n        \"\"\"Persist the collection.\n\n        This can be used to explicitly persist the data to disk.\n        It will also be called automatically when the object is destroyed.\n        \"\"\"\n        if self._persist_directory is None:\n            raise ValueError(\n                \"You must specify a persist_directory on\"\n                \"creation to persist the collection.\"\n            )\n        import chromadb\n\n        # Maintain backwards compatibility with chromadb < 0.4.0\n        major, minor, _ = chromadb.__version__.split(\".\")\n        if int(major) == 0 and int(minor) < 4:\n            self._client.persist()\n\n    def update_document(self, document_id: str, document: Document) -> None:\n        \"\"\"Update a document in the collection.\n\n        Args:\n            document_id (str): ID of the document to update.\n            document (Document): Document to update.\n        \"\"\"\n        text = document.page_content\n        metadata = document.metadata\n        if self._embedding_function is None:\n            raise ValueError(\n                \"For update, you must specify an embedding function on creation.\"\n            )\n        embeddings = self._embedding_function.embed_documents([text])\n\n        self._collection.update(\n            ids=[document_id],\n            embeddings=embeddings,\n            documents=[text],\n            metadatas=[metadata],\n        )\n\n    @classmethod\n    def from_texts(\n        cls: Type[Chroma],\n        texts: List[str],\n        embedding: Optional[Embeddings] = None,\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        client: Optional[chromadb.Client] = None,\n        collection_metadata: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> Chroma:\n        \"\"\"Create a Chroma vectorstore from a raw documents.\n\n        If a persist_directory is specified, the collection will be persisted there.\n        Otherwise, the data will be ephemeral in-memory.\n\n        Args:\n            texts (List[str]): List of texts to add to the collection.\n            collection_name (str): Name of the collection to create.\n            persist_directory (Optional[str]): Directory to persist the collection.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n            collection_metadata (Optional[Dict]): Collection configurations.\n                                                  Defaults to None.\n\n        Returns:\n            Chroma: Chroma vectorstore.\n        \"\"\"\n        chroma_collection = cls(\n            collection_name=collection_name,\n            embedding_function=embedding,\n            persist_directory=persist_directory,\n            client_settings=client_settings,\n            client=client,\n            collection_metadata=collection_metadata,\n            **kwargs,\n        )\n        chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n        return chroma_collection\n\n    @classmethod\n    def from_documents(\n        cls: Type[Chroma],\n        documents: List[Document],\n        embedding: Optional[Embeddings] = None,\n        ids: Optional[List[str]] = None,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        client: Optional[chromadb.Client] = None,  # Add this line\n        collection_metadata: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> Chroma:\n        \"\"\"Create a Chroma vectorstore from a list of documents.\n\n        If a persist_directory is specified, the collection will be persisted there.\n        Otherwise, the data will be ephemeral in-memory.\n\n        Args:\n            collection_name (str): Name of the collection to create.\n            persist_directory (Optional[str]): Directory to persist the collection.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            documents (List[Document]): List of documents to add to the vectorstore.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n            collection_metadata (Optional[Dict]): Collection configurations.\n                                                  Defaults to None.\n\n        Returns:\n            Chroma: Chroma vectorstore.\n        \"\"\"\n        texts = [doc.page_content for doc in documents]\n        metadatas = [doc.metadata for doc in documents]\n        return cls.from_texts(\n            texts=texts,\n            embedding=embedding,\n            metadatas=metadatas,\n            ids=ids,\n            collection_name=collection_name,\n            persist_directory=persist_directory,\n            client_settings=client_settings,\n            client=client,\n            collection_metadata=collection_metadata,\n            **kwargs,\n        )\n\n    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:\n        \"\"\"Delete by vector IDs.\n\n        Args:\n            ids: List of ids to delete.\n        \"\"\"\n        self._collection.delete(ids=ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>参考3中distance定义 https://github.com/nmslib/hnswlib/tree/master#python-bindings</font>\n# Distance\tparameter\tEquation\n# Squared L2\t'l2'\td = sum((Ai-Bi)^2)\n# Inner product\t'ip'\td = 1.0 - sum(Ai * Bi)\n# Cosine similarity\t'cosine'\td = 1.0 - sum(Ai * Bi) / sqrt(sum(Ai * Ai) * sum(Bi * Bi))\n# Note that inner product is not an actual metric. An element can be closer to some other element than to itself. That allows some speedup if you remove all elements that are not the closest to themselves from the index\n# 一般不用inner product 也就是ip ","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n#retriever = db.as_retriever()\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#db","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n    就回答不知道，不要编造答案.\n\n    {context}\n\n    {history}\n    问: {question}\n    答:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\nmemory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        print(res)\n        print(\"*******\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS INFO---------------------------\")\n            print(f\"score_threshold={score_threshold}\")\n            for document in docs:\n                print(\"\\n> [来源文档]: \" + document.metadata[\"source\"] )\n                print(\"> [cosine相似度得分 (0-1之间越高越相似)]:\" + str(1.0-document.metadata[\"distances\"]) )\n                print(\">[文档片段]:\" + document.page_content)\n            if len(docs)==0:\n                print(\"没有从知识库中搜索到关联信息 上面的答案answer需要重新组织 很可能是不准确的\")\n            #print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 20230901 最新","metadata":{}},{"cell_type":"markdown","source":"# 拉代码","metadata":{}},{"cell_type":"code","source":"#rm -rf /kaggle/working/*\n%cd /kaggle/working\n!git clone -b localGPT_0831_langchain_v02  https://github.com/valkryhx/localGPT","metadata":{"execution":{"iopub.status.busy":"2023-09-06T02:16:06.720164Z","iopub.execute_input":"2023-09-06T02:16:06.720604Z","iopub.status.idle":"2023-09-06T02:16:07.865519Z","shell.execute_reply.started":"2023-09-06T02:16:06.720569Z","shell.execute_reply":"2023-09-06T02:16:07.863793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 检查分支","metadata":{}},{"cell_type":"code","source":"%cd localGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-09-06T02:16:51.580615Z","iopub.execute_input":"2023-09-06T02:16:51.581135Z","iopub.status.idle":"2023-09-06T02:16:52.708539Z","shell.execute_reply.started":"2023-09-06T02:16:51.581094Z","shell.execute_reply":"2023-09-06T02:16:52.706819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-06T02:17:03.020150Z","iopub.execute_input":"2023-09-06T02:17:03.021327Z","iopub.status.idle":"2023-09-06T02:19:33.195194Z","shell.execute_reply.started":"2023-09-06T02:17:03.021242Z","shell.execute_reply":"2023-09-06T02:19:33.193776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 事先构建一个Chroma db  目前已经支持在SOURCE_DOCUMENTS 目录下新建嵌套目录和文件\n# https://github.com/valkryhx/localGPT/blob/localGPT_0831_langchain_v02/ingest.py#L131\n# ingest的 131和132行分别按照不同粒度大小切分\n","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:46:20.147463Z","iopub.execute_input":"2023-09-05T11:46:20.147845Z","iopub.status.idle":"2023-09-05T11:47:27.001172Z","shell.execute_reply.started":"2023-09-05T11:46:20.147812Z","shell.execute_reply":"2023-09-05T11:47:27.000035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 查看或者删除DB  方便更新文件后重建\n","metadata":{}},{"cell_type":"code","source":"# !ls ./DB\n# !rm -rf DB","metadata":{"execution":{"iopub.status.busy":"2023-09-01T06:14:13.327171Z","iopub.execute_input":"2023-09-01T06:14:13.328435Z","iopub.status.idle":"2023-09-01T06:14:15.594703Z","shell.execute_reply.started":"2023-09-01T06:14:13.328392Z","shell.execute_reply":"2023-09-01T06:14:15.593334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import  注意MyChroma.py 的 MyChroma class也要 import\n# 由于MyChroma中有from __future__ import annotations（不能随便删）   from __future__ 这种import必须放在第一句","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n# codes are just from local_GPT.py\n\nimport logging\n\nimport click\nimport torch\n#from auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory ,ConversationBufferWindowMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n#from langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n  # 注意这一句 是import自己自定义的MyChroma\nfrom MyChroma import MyChroma\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:47:48.197102Z","iopub.execute_input":"2023-09-05T11:47:48.198149Z","iopub.status.idle":"2023-09-05T11:48:00.447521Z","shell.execute_reply.started":"2023-09-05T11:47:48.198093Z","shell.execute_reply":"2023-09-05T11:48:00.446522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 加载词向量模型 \n# 加载Chroma向量数据库\n# 设置db的相似度metric为cosine distance\n# 设置retriever的search类型为带threshold的 设置最多返回的k（结果集大小）为3","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n#retriever = db.as_retriever()\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:48:04.863259Z","iopub.execute_input":"2023-09-05T11:48:04.863651Z","iopub.status.idle":"2023-09-05T11:48:07.554304Z","shell.execute_reply.started":"2023-09-05T11:48:04.863619Z","shell.execute_reply":"2023-09-05T11:48:07.553193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 加载llm chatglm2-6b","metadata":{}},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:48:12.268606Z","iopub.execute_input":"2023-09-05T11:48:12.268988Z","iopub.status.idle":"2023-09-05T11:50:51.587352Z","shell.execute_reply.started":"2023-09-05T11:48:12.268953Z","shell.execute_reply":"2023-09-05T11:50:51.586436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 定义prompt template 主要是选择上history、context、question字段拼接\n#  <font color=red>这个memory很重要 目前我们使用窗口为2 也就是最多存两条memroy的ConversationBufferWindowMemory\n    参考其他的memroy https://zhuanlan.zhihu.com/p/646852594qa(query)\n    另外要注意的是\n    template = \"\"\"现提供如下信息:\n    history={memory_history}\n    context={context}\n    请使用上述信息回答:{question}\"\"\"\n\n    这个模板中 {context} 这个str名不能改 这是  StuffDocumentsChain的llm_chain input_variables这个list 中必须包含的 \n    因为从db搜索回的结果一定要传给这个固定名字的变量。PromptTemplate(input_variables=[ \"context\"...])中也一定要有这个str 'context'\n\n    这个模板中的{history}其实是memory的memory_key 字段 所以可以把{history}换成任意的str 比如{memory_history} 我已经换了  PromptTemplate(input_variables 也要换成对应的\n    这个模板中的{question}是memory的input_key,根据memory的input_key 也可以随便换 PromptTemplate(input_variables也要对应换成一致即可 这个memory的input_key 必须要有 不然后面代码中的qa(query) 不能正常查询db\n</font>\n<font color=red>也可以在template中不使用{memory_history} \n 这个字段的值不是chatglm的model.chat产生的那个history 而是langchain的memory 是整齐的由memory保存的只带有question 和 answer 格式友好的历史对话 例如 k = 2时 ，memory_history为\n\nHuman:飞机票报销\nAI: 1. 国内LTC项目管理系统的负责人是谁？ 答：请联系雷彪 (blei@fiberhome.com)\n2. 谁负责管理国内LTC项目管理系统？ 答：请联系雷彪 (blei@fiberhome.com)\n3. 国内LTC项目管理系统的创始人是谁？ 答：请联系雷彪 (blei@fiberhome.com)\n4. 谁负责维护国内LTC项目管理系统？ 答：请联系雷彪 (blei@fiberhome.com)\n5. 国内LTC项目管理系统的拥有者是谁？ 答：请联系雷彪 (blei@fiberhome.com)\n\nHuman: 驻外怎么报销\nAI: 驻外人员日常费用报销的流程是什么？\", \"input\": \"\", \"output\": \"1.提交费用报销申请单，选择\"驻外\"费用类别，并填写相关信息。\\n2.选择要报销的发票，并提交。\\n3.负责人审批后，生成报销单，并交还给驻外人员。\\n4.驻外人员携带发票和报销单回公司办理报销手续。  \n    \n</font>\n# 如果不使用memroy维护的格式友好的history 那就在template中把history={memory_history} 删除\n# 但是我觉得由chatglm.chat产生的history包含了搜索信息 很杂乱 最好不用那个杂乱的history ，宁可在template里面写上history={memory_history\n\n# 我修改了https://github.com/valkryhx/localGPT/blob/localGPT_0831_langchain_v02/my_chatglm_llm.py#L74\n# 这个my_chatglm_llm 的产生history的方式 目前是根据template格式切出来最后一个提问和回答 不带乱七八糟的search doc 上下文\n# 定义RetrievalQA.from_chain_type","metadata":{}},{"cell_type":"markdown","source":"#  memory = ConversationBufferWindowMemory(k=2 ...) 只记住最近的2条历史对话","metadata":{}},{"cell_type":"code","source":"# template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n#     就回答不知道，不要编造答案.\n#     history={history}\n#     context={context}\n\n#     问: {question}\n#     答:\"\"\"\n\n\ntemplate = \"\"\"现提供如下信息:\n    history={memory_history}\n    context={context}\n    请使用上述信息回答:{question}\"\"\"\n\nprompt = PromptTemplate(input_variables=[ \"question\",'memory_history',\"context\",], template=template)\n#memory = ConversationBufferMemory(input_key=\"question\", memory_key='history')\n# https://zhuanlan.zhihu.com/p/646852594\n#一开始用这个memory = ConversationBufferWindowMemory(k=2,input_key=\"question\", memory_key='memory_history')\n\n# 参考 https://python.langchain.com/docs/modules/memory/types/buffer_window 加了return_message=True\nmemory = ConversationBufferWindowMemory(k=3,input_key=\"question\", memory_key='memory_history',return_messages=True)\n\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"execution":{"iopub.status.busy":"2023-09-05T11:53:46.120247Z","iopub.execute_input":"2023-09-05T11:53:46.120641Z","iopub.status.idle":"2023-09-05T11:53:46.131953Z","shell.execute_reply.started":"2023-09-05T11:53:46.120610Z","shell.execute_reply":"2023-09-05T11:53:46.130931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 运行","metadata":{}},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n       \n        print(\"res=\",res)\n        my_memory = memory.load_memory_variables({})\n        \n        # memory参数为4 但是这儿可以强行只保留最后2段qa\n        retain = 2\n        retain *=2 # 必须的\n        my_memory[\"memory_history\"]=my_memory[\"memory_history\"][-retain:]\n\n        print(f\"\\nmy_memory={my_memory}\")\n        print(f\"len_q_a_of_memory={len(my_memory['memory_history'])}\")\n        print(\"*******\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS INFO---------------------------\")\n            print(f\"score_threshold={score_threshold}\")\n            for document in docs:\n                print(\"\\n> [来源文档]: \" + document.metadata[\"source\"] )\n                print(\"> [cosine相似度得分 (0-1之间越高越相似)]:\" + str(1.0-document.metadata[\"distances\"]) )\n                print(\">[文档片段]:\" + document.page_content)\n            if len(docs)==0:\n                print(\"没有从知识库中搜索到关联信息 上面的答案answer需要重新组织 很可能是不准确的\")\n            #print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:28:25.154116Z","iopub.execute_input":"2023-09-05T12:28:25.154507Z","iopub.status.idle":"2023-09-05T12:30:58.603238Z","shell.execute_reply.started":"2023-09-05T12:28:25.154476Z","shell.execute_reply":"2023-09-05T12:30:58.602170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 我们来看一看怎么从my_memory这个对象中拿到标准的history standard_history","metadata":{}},{"cell_type":"code","source":"my_memory","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:31:01.618286Z","iopub.execute_input":"2023-09-05T12:31:01.618652Z","iopub.status.idle":"2023-09-05T12:31:01.626186Z","shell.execute_reply.started":"2023-09-05T12:31:01.618622Z","shell.execute_reply":"2023-09-05T12:31:01.625206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# m = my_memory.get(\"memory_history\",None)\nlist_m = [human_msg.content for human_msg in my_memory[\"memory_history\"]]\n# 注意 step =2  每隔一个取出一对 qa 保存到standard_history\nstandard_history = [[list_m[idx],list_m[idx+1]]  for idx  in range(0,len(list_m),2)]\nprint(standard_history)    ","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:31:20.037890Z","iopub.execute_input":"2023-09-05T12:31:20.038371Z","iopub.status.idle":"2023-09-05T12:31:20.047156Z","shell.execute_reply.started":"2023-09-05T12:31:20.038330Z","shell.execute_reply":"2023-09-05T12:31:20.046190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 注意这是让memory强制只要最后一对qa的写法 注意是必须为一对一对的 也就是 2 4 6。。。 所以retain乘以2","metadata":{}},{"cell_type":"code","source":"retain = 1\nretain *=2 # 必须的\nmy_memory[\"memory_history\"]=my_memory[\"memory_history\"][-retain:]","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:31:55.014968Z","iopub.execute_input":"2023-09-05T12:31:55.015479Z","iopub.status.idle":"2023-09-05T12:31:55.019680Z","shell.execute_reply.started":"2023-09-05T12:31:55.015442Z","shell.execute_reply":"2023-09-05T12:31:55.018725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_memory","metadata":{"execution":{"iopub.status.busy":"2023-09-05T12:31:59.338562Z","iopub.execute_input":"2023-09-05T12:31:59.338920Z","iopub.status.idle":"2023-09-05T12:31:59.345494Z","shell.execute_reply.started":"2023-09-05T12:31:59.338890Z","shell.execute_reply":"2023-09-05T12:31:59.344495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install aiofiles","metadata":{"execution":{"iopub.status.busy":"2023-09-06T02:23:31.008426Z","iopub.execute_input":"2023-09-06T02:23:31.009315Z","iopub.status.idle":"2023-09-06T02:23:45.418629Z","shell.execute_reply.started":"2023-09-06T02:23:31.009242Z","shell.execute_reply":"2023-09-06T02:23:45.416857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install python-multipart","metadata":{"execution":{"iopub.status.busy":"2023-09-06T02:20:34.380534Z","iopub.execute_input":"2023-09-06T02:20:34.381093Z","iopub.status.idle":"2023-09-06T02:20:50.639130Z","shell.execute_reply.started":"2023-09-06T02:20:34.381050Z","shell.execute_reply":"2023-09-06T02:20:50.636980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastapi import FastAPI, File, UploadFile\nimport uvicorn\nimport aiofiles\nimport os\napp = FastAPI()\n\n\nfolder_path = \"TEST_FASTAPI_0906\"\nif not os.path.exists(folder_path):\n    os.makedirs(folder_path)\n\n@app.post(\"/upload-file\")\nasync def create_upload_file(file: UploadFile = File(...)):\n    print(\"filename = \", file.filename) # getting filename\n    global folder_path\n    destination_file_path = os.path.join(folder_path,file.filename) # location to store file\n    async with aiofiles.open(destination_file_path, 'wb') as out_file:\n        while content := await file.read(1024):  # async read file chunk\n            await out_file.write(content)  # async write file chunk\n\n    return {\"Result\": \"OK\"}\n\nif __name__ == '__main__':\n    uvicorn.run(app, host='127.0.0.1', port=8005)\n    print(\"running\")","metadata":{"execution":{"iopub.status.busy":"2023-09-06T02:24:10.281129Z","iopub.execute_input":"2023-09-06T02:24:10.281711Z","iopub.status.idle":"2023-09-06T02:24:11.519981Z","shell.execute_reply.started":"2023-09-06T02:24:10.281668Z","shell.execute_reply":"2023-09-06T02:24:11.518413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}