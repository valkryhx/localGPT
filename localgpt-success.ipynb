{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/localGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd localGPT/\n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat /kaggle/working/localGPT/constants.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 上面这个max_se_length 其实是https://github.com/langchain-ai/langchain/blob/0689628489967785f3a11a9f29d8f6f90930f4f4/libs/langchain/langchain/embeddings/huggingface.py#L231C9-L231C65\nBreadcrumbslangchain/libs/langchain/langchain/embeddings\n/huggingface.py 231行sentence_transformers.SentenceTransformer 加载SentenceTransformer 默认的512  要动的话需要动源码","metadata":{}},{"cell_type":"markdown","source":"# ls DB/  ingest 之后的向量文件在这里  如果不想要就 rm -rf DB 那下次就要重新ingest","metadata":{}},{"cell_type":"code","source":"#run_localGPT.py 中的query=input()无法在kaggle环境上正常执行 我们自己把代码拿出来跑\n#!python run_localGPT.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# codes are just from local_GPT.py\nimport logging\n\nimport click\nimport torch\nfrom auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h=[(\"123\"),(\"3\")]\nsum([len(item) for item in h])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity","metadata":{}},{"cell_type":"code","source":"\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = Chroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n    )\n#retriever = db.as_retriever()\n\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":0.75}\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n    就回答不知道，不要编造答案.\n\n    {context}\n\n    {history}\n    问: {question}\n    答:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\nmemory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        print(\"12345\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n            for document in docs:\n                print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n                print(document.page_content)\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Chroma和FAISS默认的相似度计算metric是L2 distance\n在Chroma.from_documents方法中加入参数\ncollection_metadata={\"hnsw:space\": \"cosine\"} 改成cosine 相似度 在[0,1]之间 越高说明两个向量越相似\n\nIs you are using Chroma, you should set the distance metric when creating a collection: https://docs.trychroma.com/usage-guide#changing-the-distance-function\n\nThe default distance is l2. That is why for me it used to give scores like 3626.016357421875 when using the function similarity_search_with_relevance_scores(). On changing it to cosine, the scores are now between (0, 1] with scores closer to 1 depicting higher similarity.\n\nChroma.from_documents(documents=documents, embedding=cohere, collection_metadata={\"hnsw:space\": \"cosine\"})\n\n参考 https://stackoverflow.com/questions/76678783/langchains-chroma-vectordb-similarity-search-with-score-and-vectordb-simil\n参考 langchain.vectorstors.Chroma 源码  https://api.python.langchain.com/en/latest/_modules/langchain/vectorstores/chroma.html\n的class Chroma类的__init__方法","metadata":{}},{"cell_type":"markdown","source":"# https://github.com/langchain-ai/langchain/issues/6481\n# https://docs.trychroma.com/usage-guide#changing-the-distance-function\n#注意这个 https://stackoverflow.com/questions/76678783/langchains-chroma-vectordb-similarity-search-with-score-and-vectordb-simil\n# https://github.com/langchain-ai/langchain/issues/5458  \n这个写了怎么在向量计算结果中加入相似度阈值和个数阈值进行过滤 结果不超过k个 并且score_threshold要大于这个0.5才参与候选\nretriever=db.as_retriever(search_type=\"similarity_score_threshold\", \n                          search_kwargs={\"k\":3, \"score_threshold\":0.5})    \nhttps://github.com/langchain-ai/langchain/blob/e60e1cdf23ad73b2e0a40034c0ddfc3c8b0c9c4d/libs/langchain/langchain/vectorstores/base.py#L460","metadata":{}},{"cell_type":"code","source":"# 用新版本的https://github.com/valkryhx/localGPT   \n# branch localGPT_0831_langchain_v02","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf /kaggle/working/localGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b localGPT_0831_langchain_v02 https://github.com/valkryhx/localGPT  ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:00:43.677854Z","iopub.execute_input":"2023-08-31T15:00:43.678154Z","iopub.status.idle":"2023-08-31T15:00:44.651206Z","shell.execute_reply.started":"2023-08-31T15:00:43.678127Z","shell.execute_reply":"2023-08-31T15:00:44.649867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd localGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:00:54.341496Z","iopub.execute_input":"2023-08-31T15:00:54.341925Z","iopub.status.idle":"2023-08-31T15:00:55.505338Z","shell.execute_reply.started":"2023-08-31T15:00:54.341890Z","shell.execute_reply":"2023-08-31T15:00:55.503778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:08:07.731688Z","iopub.execute_input":"2023-08-31T15:08:07.732080Z","iopub.status.idle":"2023-08-31T15:08:26.754125Z","shell.execute_reply.started":"2023-08-31T15:08:07.732048Z","shell.execute_reply":"2023-08-31T15:08:26.752705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip list","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:07:35.511306Z","iopub.execute_input":"2023-08-31T15:07:35.514884Z","iopub.status.idle":"2023-08-31T15:07:35.521991Z","shell.execute_reply.started":"2023-08-31T15:07:35.514837Z","shell.execute_reply":"2023-08-31T15:07:35.520816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:11:05.875770Z","iopub.execute_input":"2023-08-31T15:11:05.876191Z","iopub.status.idle":"2023-08-31T15:11:37.335678Z","shell.execute_reply.started":"2023-08-31T15:11:05.876155Z","shell.execute_reply":"2023-08-31T15:11:37.334359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 重新生成   \n#ls DB\n#!rm -rf DB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# codes are just from local_GPT.py\nimport logging\n\nimport click\nimport torch\n#from auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:12:11.283632Z","iopub.execute_input":"2023-08-31T15:12:11.284045Z","iopub.status.idle":"2023-08-31T15:12:17.926043Z","shell.execute_reply.started":"2023-08-31T15:12:11.284014Z","shell.execute_reply":"2023-08-31T15:12:17.924924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"根据这个问题\n\nhttps://github.com/langchain-ai/langchain/issues/4710\n\nhttps://github.com/langchain-ai/langchain/issues/5416\n\n改的_results_to_docs_and_scores方法中\nmetadata={**result[1],**{\"distances\":result[2]} } or {})","metadata":{}},{"cell_type":"code","source":"\"\"\"Wrapper around ChromaDB embeddings platform.\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport uuid\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n)\n\nimport numpy as np\n\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.utils import xor_args\nfrom langchain.vectorstores.base import VectorStore\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\n\nif TYPE_CHECKING:\n    import chromadb\n    import chromadb.config\n    from chromadb.api.types import ID, OneOrMany, Where, WhereDocument\n\nlogger = logging.getLogger()\nDEFAULT_K = 4  # Number of Documents to return.\n\n\ndef _results_to_docs(results: Any) -> List[Document]:\n    return [doc for doc, _ in _results_to_docs_and_scores(results)]\n\n\ndef _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\n    return [\n        # TODO: Chroma can do batch querying,\n        # we shouldn't hard code to the 1st result\n        \n        # merge two dicts ,metadata = {**d1,**d2}\n        (Document(page_content=result[0], metadata={**result[1],**{\"distances\":result[2]} } or {}), result[2])\n        for result in zip(\n            results[\"documents\"][0],\n            results[\"metadatas\"][0],\n            results[\"distances\"][0],\n        )\n    ]\n\n\nclass MyChroma(VectorStore):\n    \"\"\"Wrapper around ChromaDB embeddings platform.\n\n    To use, you should have the ``chromadb`` python package installed.\n\n    Example:\n        .. code-block:: python\n\n                from langchain.vectorstores import Chroma\n                from langchain.embeddings.openai import OpenAIEmbeddings\n\n                embeddings = OpenAIEmbeddings()\n                vectorstore = Chroma(\"langchain_store\", embeddings)\n    \"\"\"\n\n    _LANGCHAIN_DEFAULT_COLLECTION_NAME = \"langchain\"\n\n    def __init__(\n        self,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        embedding_function: Optional[Embeddings] = None,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        collection_metadata: Optional[Dict] = None,\n        client: Optional[chromadb.Client] = None,\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\n    ) -> None:\n        \"\"\"Initialize with Chroma client.\"\"\"\n        try:\n            import chromadb\n            import chromadb.config\n        except ImportError:\n            raise ValueError(\n                \"Could not import chromadb python package. \"\n                \"Please install it with `pip install chromadb`.\"\n            )\n\n        if client is not None:\n            self._client_settings = client_settings\n            self._client = client\n            self._persist_directory = persist_directory\n        else:\n            if client_settings:\n                # If client_settings is provided with persist_directory specified,\n                # then it is \"in-memory and persisting to disk\" mode.\n                client_settings.persist_directory = (\n                    persist_directory or client_settings.persist_directory\n                )\n                if client_settings.persist_directory is not None:\n                    # Maintain backwards compatibility with chromadb < 0.4.0\n                    major, minor, _ = chromadb.__version__.split(\".\")\n                    if int(major) == 0 and int(minor) < 4:\n                        client_settings.chroma_db_impl = \"duckdb+parquet\"\n\n                _client_settings = client_settings\n            elif persist_directory:\n                # Maintain backwards compatibility with chromadb < 0.4.0\n                major, minor, _ = chromadb.__version__.split(\".\")\n                if int(major) == 0 and int(minor) < 4:\n                    _client_settings = chromadb.config.Settings(\n                        chroma_db_impl=\"duckdb+parquet\",\n                    )\n                else:\n                    _client_settings = chromadb.config.Settings(is_persistent=True)\n                _client_settings.persist_directory = persist_directory\n            else:\n                _client_settings = chromadb.config.Settings()\n            self._client_settings = _client_settings\n            self._client = chromadb.Client(_client_settings)\n            self._persist_directory = (\n                _client_settings.persist_directory or persist_directory\n            )\n\n        self._embedding_function = embedding_function\n        self._collection = self._client.get_or_create_collection(\n            name=collection_name,\n            embedding_function=self._embedding_function.embed_documents\n            if self._embedding_function is not None\n            else None,\n            metadata=collection_metadata,\n        )\n        self.override_relevance_score_fn = relevance_score_fn\n\n    @property\n    def embeddings(self) -> Optional[Embeddings]:\n        return self._embedding_function\n\n    @xor_args((\"query_texts\", \"query_embeddings\"))\n    def __query_collection(\n        self,\n        query_texts: Optional[List[str]] = None,\n        query_embeddings: Optional[List[List[float]]] = None,\n        n_results: int = 4,\n        where: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Query the chroma collection.\"\"\"\n        try:\n            import chromadb  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"Could not import chromadb python package. \"\n                \"Please install it with `pip install chromadb`.\"\n            )\n        return self._collection.query(\n            query_texts=query_texts,\n            query_embeddings=query_embeddings,\n            n_results=n_results,\n            where=where,\n            **kwargs,\n        )\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            ids (Optional[List[str]], optional): Optional list of IDs.\n\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        # TODO: Handle the case where the user doesn't provide ids on the Collection\n        if ids is None:\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n        texts = list(texts)\n        if self._embedding_function is not None:\n            embeddings = self._embedding_function.embed_documents(texts)\n        if metadatas:\n            # fill metadatas with empty dicts if somebody\n            # did not specify metadata for all texts\n            length_diff = len(texts) - len(metadatas)\n            if length_diff:\n                metadatas = metadatas + [{}] * length_diff\n            empty_ids = []\n            non_empty_ids = []\n            for idx, m in enumerate(metadatas):\n                if m:\n                    non_empty_ids.append(idx)\n                else:\n                    empty_ids.append(idx)\n            if non_empty_ids:\n                metadatas = [metadatas[idx] for idx in non_empty_ids]\n                texts_with_metadatas = [texts[idx] for idx in non_empty_ids]\n                embeddings_with_metadatas = (\n                    [embeddings[idx] for idx in non_empty_ids] if embeddings else None\n                )\n                ids_with_metadata = [ids[idx] for idx in non_empty_ids]\n                try:\n                    self._collection.upsert(\n                        metadatas=metadatas,\n                        embeddings=embeddings_with_metadatas,\n                        documents=texts_with_metadatas,\n                        ids=ids_with_metadata,\n                    )\n                except ValueError as e:\n                    if \"Expected metadata value to be\" in str(e):\n                        msg = (\n                            \"Try filtering complex metadata from the document using \"\n                            \"langchain.vectorstore.utils.filter_complex_metadata.\"\n                        )\n                        raise ValueError(e.args[0] + \"\\n\\n\" + msg)\n                    else:\n                        raise e\n            if empty_ids:\n                texts_without_metadatas = [texts[j] for j in empty_ids]\n                embeddings_without_metadatas = (\n                    [embeddings[j] for j in empty_ids] if embeddings else None\n                )\n                ids_without_metadatas = [ids[j] for j in empty_ids]\n                self._collection.upsert(\n                    embeddings=embeddings_without_metadatas,\n                    documents=texts_without_metadatas,\n                    ids=ids_without_metadatas,\n                )\n        else:\n            self._collection.upsert(\n                embeddings=embeddings,\n                documents=texts,\n                ids=ids,\n            )\n        return ids\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Run similarity search with Chroma.\n\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\n        docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\n        return [doc for doc, _ in docs_and_scores]\n\n    def similarity_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to embedding vector.\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List of Documents most similar to the query vector.\n        \"\"\"\n        results = self.__query_collection(\n            query_embeddings=embedding, n_results=k, where=filter\n        )\n        return _results_to_docs(results)\n\n    def similarity_search_by_vector_with_relevance_scores(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"\n        Return docs most similar to embedding vector and similarity score.\n\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        results = self.__query_collection(\n            query_embeddings=embedding, n_results=k, where=filter\n        )\n        return _results_to_docs_and_scores(results)\n\n    def similarity_search_with_score(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Run similarity search with Chroma with distance.\n\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        if self._embedding_function is None:\n            results = self.__query_collection(\n                query_texts=[query], n_results=k, where=filter\n            )\n        else:\n            query_embedding = self._embedding_function.embed_query(query)\n            results = self.__query_collection(\n                query_embeddings=[query_embedding], n_results=k, where=filter\n            )\n\n        return _results_to_docs_and_scores(results)\n\n    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n        \"\"\"\n        The 'correct' relevance function\n        may differ depending on a few things, including:\n        - the distance / similarity metric used by the VectorStore\n        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)\n        - embedding dimensionality\n        - etc.\n        \"\"\"\n        if self.override_relevance_score_fn:\n            return self.override_relevance_score_fn\n\n        distance = \"l2\"\n        distance_key = \"hnsw:space\"\n        metadata = self._collection.metadata\n\n        if metadata and distance_key in metadata:\n            distance = metadata[distance_key]\n\n        if distance == \"cosine\":\n            return self._cosine_relevance_score_fn\n        elif distance == \"l2\":\n            return self._euclidean_relevance_score_fn\n        elif distance == \"ip\":\n            return self._max_inner_product_relevance_score_fn\n        else:\n            raise ValueError(\n                \"No supported normalization function\"\n                f\" for distance metric of type: {distance}.\"\n                \"Consider providing relevance_score_fn to Chroma constructor.\"\n            )\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n\n        results = self.__query_collection(\n            query_embeddings=embedding,\n            n_results=fetch_k,\n            where=filter,\n            include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"],\n        )\n        mmr_selected = maximal_marginal_relevance(\n            np.array(embedding, dtype=np.float32),\n            results[\"embeddings\"][0],\n            k=k,\n            lambda_mult=lambda_mult,\n        )\n\n        candidates = _results_to_docs(results)\n\n        selected_results = [r for i, r in enumerate(candidates) if i in mmr_selected]\n        return selected_results\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            query: Text to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        if self._embedding_function is None:\n            raise ValueError(\n                \"For MMR search, you must specify an embedding function on\" \"creation.\"\n            )\n\n        embedding = self._embedding_function.embed_query(query)\n        docs = self.max_marginal_relevance_search_by_vector(\n            embedding, k, fetch_k, lambda_mult=lambda_mult, filter=filter\n        )\n        return docs\n\n    def delete_collection(self) -> None:\n        \"\"\"Delete the collection.\"\"\"\n        self._client.delete_collection(self._collection.name)\n\n    def get(\n        self,\n        ids: Optional[OneOrMany[ID]] = None,\n        where: Optional[Where] = None,\n        limit: Optional[int] = None,\n        offset: Optional[int] = None,\n        where_document: Optional[WhereDocument] = None,\n        include: Optional[List[str]] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Gets the collection.\n\n        Args:\n            ids: The ids of the embeddings to get. Optional.\n            where: A Where type dict used to filter results by.\n                   E.g. `{\"color\" : \"red\", \"price\": 4.20}`. Optional.\n            limit: The number of documents to return. Optional.\n            offset: The offset to start returning results from.\n                    Useful for paging results with limit. Optional.\n            where_document: A WhereDocument type dict used to filter by the documents.\n                            E.g. `{$contains: {\"text\": \"hello\"}}`. Optional.\n            include: A list of what to include in the results.\n                     Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n                     Ids are always included.\n                     Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n        \"\"\"\n        kwargs = {\n            \"ids\": ids,\n            \"where\": where,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"where_document\": where_document,\n        }\n\n        if include is not None:\n            kwargs[\"include\"] = include\n\n        return self._collection.get(**kwargs)\n\n    def persist(self) -> None:\n        \"\"\"Persist the collection.\n\n        This can be used to explicitly persist the data to disk.\n        It will also be called automatically when the object is destroyed.\n        \"\"\"\n        if self._persist_directory is None:\n            raise ValueError(\n                \"You must specify a persist_directory on\"\n                \"creation to persist the collection.\"\n            )\n        import chromadb\n\n        # Maintain backwards compatibility with chromadb < 0.4.0\n        major, minor, _ = chromadb.__version__.split(\".\")\n        if int(major) == 0 and int(minor) < 4:\n            self._client.persist()\n\n    def update_document(self, document_id: str, document: Document) -> None:\n        \"\"\"Update a document in the collection.\n\n        Args:\n            document_id (str): ID of the document to update.\n            document (Document): Document to update.\n        \"\"\"\n        text = document.page_content\n        metadata = document.metadata\n        if self._embedding_function is None:\n            raise ValueError(\n                \"For update, you must specify an embedding function on creation.\"\n            )\n        embeddings = self._embedding_function.embed_documents([text])\n\n        self._collection.update(\n            ids=[document_id],\n            embeddings=embeddings,\n            documents=[text],\n            metadatas=[metadata],\n        )\n\n    @classmethod\n    def from_texts(\n        cls: Type[Chroma],\n        texts: List[str],\n        embedding: Optional[Embeddings] = None,\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        client: Optional[chromadb.Client] = None,\n        collection_metadata: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> Chroma:\n        \"\"\"Create a Chroma vectorstore from a raw documents.\n\n        If a persist_directory is specified, the collection will be persisted there.\n        Otherwise, the data will be ephemeral in-memory.\n\n        Args:\n            texts (List[str]): List of texts to add to the collection.\n            collection_name (str): Name of the collection to create.\n            persist_directory (Optional[str]): Directory to persist the collection.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n            collection_metadata (Optional[Dict]): Collection configurations.\n                                                  Defaults to None.\n\n        Returns:\n            Chroma: Chroma vectorstore.\n        \"\"\"\n        chroma_collection = cls(\n            collection_name=collection_name,\n            embedding_function=embedding,\n            persist_directory=persist_directory,\n            client_settings=client_settings,\n            client=client,\n            collection_metadata=collection_metadata,\n            **kwargs,\n        )\n        chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n        return chroma_collection\n\n    @classmethod\n    def from_documents(\n        cls: Type[Chroma],\n        documents: List[Document],\n        embedding: Optional[Embeddings] = None,\n        ids: Optional[List[str]] = None,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        client: Optional[chromadb.Client] = None,  # Add this line\n        collection_metadata: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> Chroma:\n        \"\"\"Create a Chroma vectorstore from a list of documents.\n\n        If a persist_directory is specified, the collection will be persisted there.\n        Otherwise, the data will be ephemeral in-memory.\n\n        Args:\n            collection_name (str): Name of the collection to create.\n            persist_directory (Optional[str]): Directory to persist the collection.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            documents (List[Document]): List of documents to add to the vectorstore.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n            collection_metadata (Optional[Dict]): Collection configurations.\n                                                  Defaults to None.\n\n        Returns:\n            Chroma: Chroma vectorstore.\n        \"\"\"\n        texts = [doc.page_content for doc in documents]\n        metadatas = [doc.metadata for doc in documents]\n        return cls.from_texts(\n            texts=texts,\n            embedding=embedding,\n            metadatas=metadatas,\n            ids=ids,\n            collection_name=collection_name,\n            persist_directory=persist_directory,\n            client_settings=client_settings,\n            client=client,\n            collection_metadata=collection_metadata,\n            **kwargs,\n        )\n\n    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:\n        \"\"\"Delete by vector IDs.\n\n        Args:\n            ids: List of ids to delete.\n        \"\"\"\n        self._collection.delete(ids=ids)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:13:23.985652Z","iopub.execute_input":"2023-08-31T15:13:23.986023Z","iopub.status.idle":"2023-08-31T15:13:24.057759Z","shell.execute_reply.started":"2023-08-31T15:13:23.985991Z","shell.execute_reply":"2023-08-31T15:13:24.056371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"ip\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n#retriever = db.as_retriever()\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:13:53.164364Z","iopub.execute_input":"2023-08-31T15:13:53.164794Z","iopub.status.idle":"2023-08-31T15:13:56.049280Z","shell.execute_reply.started":"2023-08-31T15:13:53.164762Z","shell.execute_reply":"2023-08-31T15:13:56.048268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#db","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n    就回答不知道，不要编造答案.\n\n    {context}\n\n    {history}\n    问: {question}\n    答:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\nmemory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:14:48.998624Z","iopub.execute_input":"2023-08-31T15:14:48.999024Z","iopub.status.idle":"2023-08-31T15:14:49.682629Z","shell.execute_reply.started":"2023-08-31T15:14:48.998989Z","shell.execute_reply":"2023-08-31T15:14:49.681225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        print(res)\n        print(\"*******\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS INFO---------------------------\")\n            print(f\"score_threshold={score_threshold}\")\n            for document in docs:\n                print(\"\\n> [来源文档]: \" + document.metadata[\"source\"] )\n                print(\"> [cosine相似度得分 (0-1之间越高越相似)]:\" + str(1.0-document.metadata[\"distances\"]) )\n                print(\">[文档片段]:\" + document.page_content)\n            if len(docs)==0:\n                print(\"没有从知识库中搜索到关联信息 上面的答案answer需要重新组织 很可能是不准确的\")\n            #print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}