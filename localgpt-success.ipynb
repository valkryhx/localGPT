{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/localGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd localGPT/\n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cat /kaggle/working/localGPT/constants.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 上面这个max_se_length 其实是https://github.com/langchain-ai/langchain/blob/0689628489967785f3a11a9f29d8f6f90930f4f4/libs/langchain/langchain/embeddings/huggingface.py#L231C9-L231C65\nBreadcrumbslangchain/libs/langchain/langchain/embeddings\n/huggingface.py 231行sentence_transformers.SentenceTransformer 加载SentenceTransformer 默认的512  要动的话需要动源码","metadata":{}},{"cell_type":"markdown","source":"# ls DB/  ingest 之后的向量文件在这里  如果不想要就 rm -rf DB 那下次就要重新ingest","metadata":{}},{"cell_type":"code","source":"#run_localGPT.py 中的query=input()无法在kaggle环境上正常执行 我们自己把代码拿出来跑\n#!python run_localGPT.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# codes are just from local_GPT.py\nimport logging\n\nimport click\nimport torch\nfrom auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h=[(\"123\"),(\"3\")]\nsum([len(item) for item in h])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity","metadata":{}},{"cell_type":"code","source":"\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = Chroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"cosine\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n    )\n#retriever = db.as_retriever()\n\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":0.75}\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n    就回答不知道，不要编造答案.\n\n    {context}\n\n    {history}\n    问: {question}\n    答:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\nmemory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        print(\"12345\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n            for document in docs:\n                print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n                print(document.page_content)\n            print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Chroma和FAISS默认的相似度计算metric是L2 distance\n在Chroma.from_documents方法中加入参数\ncollection_metadata={\"hnsw:space\": \"cosine\"} 改成cosine 相似度 在[0,1]之间 越高说明两个向量越相似\n\nIs you are using Chroma, you should set the distance metric when creating a collection: https://docs.trychroma.com/usage-guide#changing-the-distance-function\n\nThe default distance is l2. That is why for me it used to give scores like 3626.016357421875 when using the function similarity_search_with_relevance_scores(). On changing it to cosine, the scores are now between (0, 1] with scores closer to 1 depicting higher similarity.\n\nChroma.from_documents(documents=documents, embedding=cohere, collection_metadata={\"hnsw:space\": \"cosine\"})\n\n参考 https://stackoverflow.com/questions/76678783/langchains-chroma-vectordb-similarity-search-with-score-and-vectordb-simil\n参考 langchain.vectorstors.Chroma 源码  https://api.python.langchain.com/en/latest/_modules/langchain/vectorstores/chroma.html\n的class Chroma类的__init__方法","metadata":{}},{"cell_type":"markdown","source":"# https://github.com/langchain-ai/langchain/issues/6481\n# https://docs.trychroma.com/usage-guide#changing-the-distance-function\n#注意这个 https://stackoverflow.com/questions/76678783/langchains-chroma-vectordb-similarity-search-with-score-and-vectordb-simil\n# https://github.com/langchain-ai/langchain/issues/5458  \n这个写了怎么在向量计算结果中加入相似度阈值和个数阈值进行过滤 结果不超过k个 并且score_threshold要大于这个0.5才参与候选\nretriever=db.as_retriever(search_type=\"similarity_score_threshold\", \n                          search_kwargs={\"k\":3, \"score_threshold\":0.5})    \nhttps://github.com/langchain-ai/langchain/blob/e60e1cdf23ad73b2e0a40034c0ddfc3c8b0c9c4d/libs/langchain/langchain/vectorstores/base.py#L460","metadata":{}},{"cell_type":"markdown","source":"# langchain使用样例\nhttps://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa","metadata":{}},{"cell_type":"markdown","source":"# python 合并字典 优雅\nhttps://segmentfault.com/a/1190000010567015","metadata":{}},{"cell_type":"code","source":"# 用新版本的https://github.com/valkryhx/localGPT   \n# branch localGPT_0831_langchain_v02","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf /kaggle/working/localGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>注意 要让Chroma使用cosine distance（注意不是cosine similarity）而非默认的L2 distance</font>  \nhttps://github.com/valkryhx/localGPT/blob/localGPT_0831_langchain_v02/ingest.py#L150 加一行   collection_metadata={\"hnsw:space\": \"cosine\"},\n在下面的# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"ip\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n也加一行\n\n# 魔改了langchain/vectorstores/chroma.py 增加了distance value输出  越相似的distance越小\n# #retriever = db.as_retriever() 改了 增加 search_type=\"similarity_score_threshold\", search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )\n # 注意 这个threshold如果取0.7 那么实际对应的是distance value小于0.3的text 也就是说distance越小 则1-distance 越大 越容易超过threshold  这符合逻辑和变量名定义  后面print时 我会用1-distance的值来显示","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b localGPT_0831_langchain_v02 https://github.com/valkryhx/localGPT  ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:59:23.119684Z","iopub.execute_input":"2023-08-31T15:59:23.120339Z","iopub.status.idle":"2023-08-31T15:59:24.105418Z","shell.execute_reply.started":"2023-08-31T15:59:23.120298Z","shell.execute_reply":"2023-08-31T15:59:24.104106Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'localGPT' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd localGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:00:16.052946Z","iopub.execute_input":"2023-08-31T16:00:16.053709Z","iopub.status.idle":"2023-08-31T16:00:17.063122Z","shell.execute_reply.started":"2023-08-31T16:00:16.053669Z","shell.execute_reply":"2023-08-31T16:00:17.061940Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/localGPT\nOn branch localGPT_0831_langchain_v02\nYour branch is up to date with 'origin/localGPT_0831_langchain_v02'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\t\u001b[31mmodified:   localGPTUI/localGPTUI.py\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.rtl.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.rtl.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.rtl.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-grid.rtl.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.rtl.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.rtl.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.rtl.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-reboot.rtl.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.rtl.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.rtl.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.rtl.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap-utilities.rtl.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.rtl.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.rtl.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.rtl.min.css\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.rtl.min.css.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.bundle.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.bundle.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.bundle.min.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.bundle.min.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.esm.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.esm.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.esm.min.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.esm.min.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js.map\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/bootstrap-5.1.3-dist/js/jquery-3.2.1.min.js\u001b[m\n\t\u001b[31mmodified:   localGPTUI/static/dependencies/jquery/3.6.0/jquery.min.js\u001b[m\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t\u001b[31mDB/\u001b[m\n\t\u001b[31m__pycache__/\u001b[m\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:01:17.511868Z","iopub.execute_input":"2023-08-31T16:01:17.512686Z","iopub.status.idle":"2023-08-31T16:02:49.648785Z","shell.execute_reply.started":"2023-08-31T16:01:17.512647Z","shell.execute_reply":"2023-08-31T16:02:49.647557Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\nIgnoring protobuf: markers 'sys_platform == \"darwin\" and platform_machine != \"arm64\"' don't match your environment\nIgnoring protobuf: markers 'sys_platform == \"darwin\" and platform_machine == \"arm64\"' don't match your environment\nIgnoring bitsandbytes-windows: markers 'sys_platform == \"win32\"' don't match your environment\nCollecting langchain==0.0.277 (from -r requirements.txt (line 2))\n  Downloading langchain-0.0.277-py3-none-any.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting chromadb==0.4.6 (from -r requirements.txt (line 3))\n  Downloading chromadb-0.4.6-py3-none-any.whl (405 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20221105 (from -r requirements.txt (line 5))\n  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting InstructorEmbedding (from -r requirements.txt (line 6))\n  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\nCollecting sentence-transformers (from -r requirements.txt (line 7))\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting faiss-cpu (from -r requirements.txt (line 8))\n  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.16.4)\nRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (4.30.2)\nCollecting protobuf==3.20.0 (from -r requirements.txt (line 11))\n  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting docx2txt (from -r requirements.txt (line 15))\n  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting unstructured (from -r requirements.txt (line 16))\n  Downloading unstructured-0.10.10-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting urllib3==1.26.6 (from -r requirements.txt (line 19))\n  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate==0.21.0 (from -r requirements.txt (line 20))\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting bitsandbytes==0.41.1 (from -r requirements.txt (line 21))\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (8.1.3)\nRequirement already satisfied: flask in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (2.3.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (2.31.0)\nCollecting streamlit (from -r requirements.txt (line 28))\n  Downloading streamlit-1.26.0-py2.py3-none-any.whl (8.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting Streamlit-extras (from -r requirements.txt (line 29))\n  Downloading streamlit_extras-0.3.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (3.1.2)\nCollecting loguru (from -r requirements.txt (line 33))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (3.8.4)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (0.5.9)\nCollecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.277->-r requirements.txt (line 2))\n  Downloading langsmith-0.0.30-py3-none-any.whl (35 kB)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (2.8.4)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (1.23.5)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (1.10.10)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.277->-r requirements.txt (line 2)) (8.2.2)\nCollecting chroma-hnswlib==0.7.2 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: fastapi<0.100.0,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (0.98.0)\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (0.22.0)\nCollecting posthog>=2.4.0 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (4.6.3)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (0.13.3)\nCollecting pypika>=0.48.9 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (4.65.0)\nCollecting overrides>=7.3.1 (from chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.6->-r requirements.txt (line 3)) (5.12.0)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105->-r requirements.txt (line 5)) (3.1.0)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20221105->-r requirements.txt (line 5)) (41.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (3.12.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (2023.6.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 10)) (0.3.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 20)) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 20)) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (1.11.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 7)) (0.1.99)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 9)) (2023.6.0)\nCollecting chardet (from unstructured->-r requirements.txt (line 16))\n  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting filetype (from unstructured->-r requirements.txt (line 16))\n  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nCollecting python-magic (from unstructured->-r requirements.txt (line 16))\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 16)) (4.9.3)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 16)) (0.9.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 16)) (4.12.2)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from unstructured->-r requirements.txt (line 16)) (2.6.0)\nRequirement already satisfied: Werkzeug>=2.3.3 in /opt/conda/lib/python3.10/site-packages (from flask->-r requirements.txt (line 24)) (2.3.6)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->-r requirements.txt (line 24)) (3.1.2)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->-r requirements.txt (line 24)) (2.1.2)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask->-r requirements.txt (line 24)) (1.6.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 25)) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 25)) (2023.5.7)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (5.0.1)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (4.2.4)\nRequirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (6.7.0)\nRequirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (1.5.3)\nRequirement already satisfied: pillow<10,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (9.5.0)\nRequirement already satisfied: pyarrow>=6.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (11.0.0)\nRequirement already satisfied: pympler<2,>=0.9 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (1.0.1)\nRequirement already satisfied: python-dateutil<3,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (2.8.2)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (13.4.2)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (0.10.2)\nCollecting tzlocal<5,>=1.1 (from streamlit->-r requirements.txt (line 28))\n  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\nCollecting validators<1,>=0.2 (from streamlit->-r requirements.txt (line 28))\n  Downloading validators-0.21.2-py3-none-any.whl (25 kB)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (3.1.31)\nCollecting pydeck<1,>=0.8 (from streamlit->-r requirements.txt (line 28))\n  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit->-r requirements.txt (line 28)) (6.3.2)\nCollecting watchdog>=2.1.5 (from streamlit->-r requirements.txt (line 28))\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting htbuilder==0.6.1 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading htbuilder-0.6.1.tar.gz (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting markdownlit>=0.0.5 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading markdownlit-0.0.7-py3-none-any.whl (15 kB)\nCollecting st-annotated-text>=3.0.0 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading st-annotated-text-4.0.0.tar.gz (7.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting streamlit-camera-input-live>=0.2.0 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading streamlit_camera_input_live-0.2.0-py3-none-any.whl (6.6 kB)\nCollecting streamlit-card>=0.0.4 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading streamlit_card-0.0.61-py3-none-any.whl (680 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.5/680.5 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting streamlit-embedcode>=0.1.2 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading streamlit_embedcode-0.1.2-py3-none-any.whl (3.5 kB)\nCollecting streamlit-faker>=0.0.2 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading streamlit_faker-0.0.2-py3-none-any.whl (9.8 kB)\nCollecting streamlit-image-coordinates<0.2.0,>=0.1.1 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading streamlit_image_coordinates-0.1.6-py3-none-any.whl (6.3 kB)\nCollecting streamlit-keyup>=0.1.9 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading streamlit_keyup-0.2.0-py3-none-any.whl (7.4 kB)\nCollecting streamlit-toggle-switch>=1.0.2 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading streamlit_toggle_switch-1.0.2-py3-none-any.whl (635 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.4/635.4 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting streamlit-vertical-slider>=1.0.2 (from Streamlit-extras->-r requirements.txt (line 29))\n  Downloading streamlit_vertical_slider-1.0.2-py3-none-any.whl (624 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from htbuilder==0.6.1->Streamlit-extras->-r requirements.txt (line 29)) (9.1.0)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl->-r requirements.txt (line 32)) (1.1.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.277->-r requirements.txt (line 2)) (1.3.1)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 28)) (4.17.3)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 28)) (0.12.0)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->-r requirements.txt (line 5)) (1.15.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.277->-r requirements.txt (line 2)) (3.19.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.277->-r requirements.txt (line 2)) (1.5.1)\nRequirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.277->-r requirements.txt (line 2)) (0.9.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r requirements.txt (line 3)) (0.27.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 28)) (4.0.10)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit->-r requirements.txt (line 28)) (3.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 24)) (2.1.3)\nRequirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from markdownlit>=0.0.5->Streamlit-extras->-r requirements.txt (line 29)) (3.4.3)\nCollecting favicon (from markdownlit>=0.0.5->Streamlit-extras->-r requirements.txt (line 29))\n  Downloading favicon-0.7.0-py2.py3-none-any.whl (5.9 kB)\nCollecting pymdown-extensions (from markdownlit>=0.0.5->Streamlit-extras->-r requirements.txt (line 29))\n  Downloading pymdown_extensions-10.2.1-py3-none-any.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.1/241.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3)) (23.5.26)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3)) (1.12)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.2->-r requirements.txt (line 10)) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit->-r requirements.txt (line 28)) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.6->-r requirements.txt (line 3)) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.6->-r requirements.txt (line 3)) (2.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 28)) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 28)) (2.15.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.277->-r requirements.txt (line 2)) (2.0.2)\nCollecting faker (from streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 29))\n  Downloading Faker-19.3.1-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 29)) (3.7.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 20)) (3.1)\nCollecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit->-r requirements.txt (line 28))\n  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (0.17.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (0.19.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.6->-r requirements.txt (line 3)) (11.0.3)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->unstructured->-r requirements.txt (line 16)) (2.3.2.post1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 7)) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 7)) (3.1.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->-r requirements.txt (line 5)) (2.21)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 28)) (5.0.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 28)) (0.19.3)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r requirements.txt (line 28)) (0.1.0)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r requirements.txt (line 3)) (3.7.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.277->-r requirements.txt (line 2)) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3))\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 29)) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 29)) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 29)) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->streamlit-faker>=0.0.2->Streamlit-extras->-r requirements.txt (line 29)) (1.4.4)\nRequirement already satisfied: tzdata in /opt/conda/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit->-r requirements.txt (line 28)) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.6->-r requirements.txt (line 3)) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r requirements.txt (line 3)) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.6->-r requirements.txt (line 3)) (1.1.1)\nBuilding wheels for collected packages: chroma-hnswlib, sentence-transformers, docx2txt, htbuilder, pypika, st-annotated-text\n  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=200798 sha256=1440fde5c47b0d2033225eb5bd9603bd9a8480817a28a5beb4272ee61f1cf93b\n  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=328f060fb132a29886d95ae5e98e04c660c490c6b425b52b554bfb6634e7c7d2\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3978 sha256=e4c932eb72e4b62a57677fdbebb713d63ca739b751c41311b4c1250ca69df748\n  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n  Building wheel for htbuilder (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for htbuilder: filename=htbuilder-0.6.1-py3-none-any.whl size=12456 sha256=0a6d60d24c12f52af0afa212ae3ac4fe015a923d57baa3c05d7214694f998908\n  Stored in directory: /root/.cache/pip/wheels/12/d3/e9/e499c6b18281f756e41e385735cf3ea6980baf126f093b0ba1\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=d16280d4e97633699d194a49f34695f19e9b956ec5b3ec87527df02ac42ccb8f\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n  Building wheel for st-annotated-text (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for st-annotated-text: filename=st_annotated_text-4.0.0-py3-none-any.whl size=8923 sha256=9562cbe25fd82d03e4a8488db01959c6f1aa4faab8c6b4726877e651ef56d815\n  Stored in directory: /root/.cache/pip/wheels/75/24/e8/1c0562e6308f0fff00c6f25972c0bf0a78fbf2be2dc65ef148\nSuccessfully built chroma-hnswlib sentence-transformers docx2txt htbuilder pypika st-annotated-text\nInstalling collected packages: pypika, monotonic, InstructorEmbedding, filetype, faiss-cpu, docx2txt, bitsandbytes, watchdog, validators, urllib3, pytz-deprecation-shim, python-magic, pymdown-extensions, pulsar-client, protobuf, overrides, loguru, humanfriendly, htbuilder, chroma-hnswlib, chardet, tzlocal, st-annotated-text, pydeck, faker, coloredlogs, posthog, pdfminer.six, onnxruntime, langsmith, favicon, accelerate, streamlit, chromadb, unstructured, streamlit-vertical-slider, streamlit-toggle-switch, streamlit-keyup, streamlit-image-coordinates, streamlit-embedcode, streamlit-card, streamlit-camera-input-live, sentence-transformers, langchain, streamlit-faker, markdownlit, Streamlit-extras\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.15\n    Uninstalling urllib3-1.26.15:\n      Successfully uninstalled urllib3-1.26.15\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: overrides\n    Found existing installation: overrides 6.5.0\n    Uninstalling overrides-6.5.0:\n      Successfully uninstalled overrides-6.5.0\n  Attempting uninstall: tzlocal\n    Found existing installation: tzlocal 5.0.1\n    Uninstalling tzlocal-5.0.1:\n      Successfully uninstalled tzlocal-5.0.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.0 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndistributed 2023.3.2.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ngoogle-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-artifact-registry 1.8.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-datastore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-dlp 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-language 2.10.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-monitoring 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-pubsub 2.17.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\ngoogle-cloud-pubsub 2.17.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\ngoogle-cloud-resource-manager 1.10.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-spanner 3.36.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-translate 3.11.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogle-cloud-videointelligence 2.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngoogleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\ngrpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.14.0 requires protobuf>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nsentry-sdk 1.27.1 requires urllib3>=1.26.11; python_version >= \"3.6\", but you have urllib3 1.26.6 which is incompatible.\ntensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\ntensorflow-serving-api 2.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed InstructorEmbedding-1.0.1 Streamlit-extras-0.3.0 accelerate-0.21.0 bitsandbytes-0.41.1 chardet-5.2.0 chroma-hnswlib-0.7.2 chromadb-0.4.6 coloredlogs-15.0.1 docx2txt-0.8 faiss-cpu-1.7.4 faker-19.3.1 favicon-0.7.0 filetype-1.2.0 htbuilder-0.6.1 humanfriendly-10.0 langchain-0.0.277 langsmith-0.0.30 loguru-0.7.0 markdownlit-0.0.7 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 pdfminer.six-20221105 posthog-3.0.2 protobuf-3.20.0 pulsar-client-3.3.0 pydeck-0.8.0 pymdown-extensions-10.2.1 pypika-0.48.9 python-magic-0.4.27 pytz-deprecation-shim-0.1.0.post0 sentence-transformers-2.2.2 st-annotated-text-4.0.0 streamlit-1.26.0 streamlit-camera-input-live-0.2.0 streamlit-card-0.0.61 streamlit-embedcode-0.1.2 streamlit-faker-0.0.2 streamlit-image-coordinates-0.1.6 streamlit-keyup-0.2.0 streamlit-toggle-switch-1.0.2 streamlit-vertical-slider-1.0.2 tzlocal-4.3.1 unstructured-0.10.10 urllib3-1.26.16 validators-0.21.2 watchdog-3.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip list","metadata":{"execution":{"iopub.status.busy":"2023-08-31T15:07:35.511306Z","iopub.execute_input":"2023-08-31T15:07:35.514884Z","iopub.status.idle":"2023-08-31T15:07:35.521991Z","shell.execute_reply.started":"2023-08-31T15:07:35.514837Z","shell.execute_reply":"2023-08-31T15:07:35.520816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!python ingest.py","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:03:54.870224Z","iopub.execute_input":"2023-08-31T16:03:54.870625Z","iopub.status.idle":"2023-08-31T16:04:49.503153Z","shell.execute_reply.started":"2023-08-31T16:03:54.870593Z","shell.execute_reply":"2023-08-31T16:04:49.501612Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nDownloading (…)3c64c/.gitattributes: 100%|█| 1.53k/1.53k [00:00<00:00, 7.29MB/s]\nDownloading (…)_Pooling/config.json: 100%|█████| 190/190 [00:00<00:00, 1.04MB/s]\nDownloading (…)51cbe3c64c/README.md: 100%|█| 26.6k/26.6k [00:00<00:00, 25.2MB/s]\nDownloading (…)cbe3c64c/config.json: 100%|█████| 932/932 [00:00<00:00, 5.32MB/s]\nDownloading model.safetensors: 100%|██████████| 409M/409M [00:02<00:00, 187MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 409M/409M [00:02<00:00, 178MB/s]\nDownloading (…)nce_bert_config.json: 100%|████| 53.0/53.0 [00:00<00:00, 363kB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████| 125/125 [00:00<00:00, 771kB/s]\nDownloading (…)3c64c/tokenizer.json: 100%|███| 439k/439k [00:00<00:00, 7.76MB/s]\nDownloading (…)okenizer_config.json: 100%|█████| 342/342 [00:00<00:00, 2.19MB/s]\nDownloading (…)51cbe3c64c/vocab.txt: 100%|███| 110k/110k [00:00<00:00, 1.28MB/s]\nDownloading (…)be3c64c/modules.json: 100%|█████| 229/229 [00:00<00:00, 1.43MB/s]\nload INSTRUCTOR_Transformer\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"code","source":"# 重新生成   \n#ls DB\n#!rm -rf DB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# codes are just from local_GPT.py\nimport logging\n\nimport click\nimport torch\n#from auto_gptq import AutoGPTQForCausalLM\nfrom huggingface_hub import hf_hub_download\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.llms import HuggingFacePipeline, LlamaCpp\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\n# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.vectorstores import Chroma\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    LlamaForCausalLM,\n    LlamaTokenizer,\n    pipeline,\n)\n\nfrom constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:04:58.589999Z","iopub.execute_input":"2023-08-31T16:04:58.590419Z","iopub.status.idle":"2023-08-31T16:05:09.196405Z","shell.execute_reply.started":"2023-08-31T16:04:58.590384Z","shell.execute_reply":"2023-08-31T16:05:09.195365Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# 这里会下载和加载chatglm2-6b\n!git pull --all --force\nfrom my_chatglm_llm import ChatGLM","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:05:16.665077Z","iopub.execute_input":"2023-08-31T16:05:16.665525Z","iopub.status.idle":"2023-08-31T16:08:11.935959Z","shell.execute_reply.started":"2023-08-31T16:05:16.665488Z","shell.execute_reply":"2023-08-31T16:08:11.934953Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1d0ce1de6654d74b8291048c2b59708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e185fe139ba4007bc95e6d110bdcf53"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23f4143102f54c4e999524afe1558f0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c424aa15379749e4b6a6e0bdcf196896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00c6c8df3d134f5993998c2d2584e506"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd441a68038b40d0bc746c05b5b9d12d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"994e8b5c276b420dba93cda43261b5b6"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da697492eff14e34bc8b1a05f2e8cecf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24fb16365604a3db10349fdb9b2700a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42dbd4b6e2444098b11fc1c7dc88d493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5785f2750526480ba45fcc833bf803ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81e104b4553f463caabe0e9842193d5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf995f5b3ac4788bb80f0f877942be1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b4d9222ece4ba6bfb58333b7083421"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3881aa3aab5f4737874f1f8cf2c0c4c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f38066de248345f1b99eabc4ee5003b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2995d5b5ee4f4af094e38390f188878b"}},"metadata":{}}]},{"cell_type":"markdown","source":"根据这个问题\n\nhttps://github.com/langchain-ai/langchain/issues/4710\n\nhttps://github.com/langchain-ai/langchain/issues/5416\n\n改的_results_to_docs_and_scores方法中\nmetadata={**result[1],**{\"distances\":result[2]} } or {})","metadata":{}},{"cell_type":"code","source":"\"\"\"Wrapper around ChromaDB embeddings platform.\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport uuid\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n)\n\nimport numpy as np\n\nfrom langchain.docstore.document import Document\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.utils import xor_args\nfrom langchain.vectorstores.base import VectorStore\nfrom langchain.vectorstores.utils import maximal_marginal_relevance\n\nif TYPE_CHECKING:\n    import chromadb\n    import chromadb.config\n    from chromadb.api.types import ID, OneOrMany, Where, WhereDocument\n\nlogger = logging.getLogger()\nDEFAULT_K = 4  # Number of Documents to return.\n\n\ndef _results_to_docs(results: Any) -> List[Document]:\n    return [doc for doc, _ in _results_to_docs_and_scores(results)]\n\n\ndef _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\n    return [\n        # TODO: Chroma can do batch querying,\n        # we shouldn't hard code to the 1st result\n        \n        # merge two dicts ,metadata = {**d1,**d2}\n        (Document(page_content=result[0], metadata={**result[1],**{\"distances\":result[2]} } or {}), result[2])\n        for result in zip(\n            results[\"documents\"][0],\n            results[\"metadatas\"][0],\n            results[\"distances\"][0],\n        )\n    ]\n\n\nclass MyChroma(VectorStore):\n    \"\"\"Wrapper around ChromaDB embeddings platform.\n\n    To use, you should have the ``chromadb`` python package installed.\n\n    Example:\n        .. code-block:: python\n\n                from langchain.vectorstores import Chroma\n                from langchain.embeddings.openai import OpenAIEmbeddings\n\n                embeddings = OpenAIEmbeddings()\n                vectorstore = Chroma(\"langchain_store\", embeddings)\n    \"\"\"\n\n    _LANGCHAIN_DEFAULT_COLLECTION_NAME = \"langchain\"\n\n    def __init__(\n        self,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        embedding_function: Optional[Embeddings] = None,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        collection_metadata: Optional[Dict] = None,\n        client: Optional[chromadb.Client] = None,\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\n    ) -> None:\n        \"\"\"Initialize with Chroma client.\"\"\"\n        try:\n            import chromadb\n            import chromadb.config\n        except ImportError:\n            raise ValueError(\n                \"Could not import chromadb python package. \"\n                \"Please install it with `pip install chromadb`.\"\n            )\n\n        if client is not None:\n            self._client_settings = client_settings\n            self._client = client\n            self._persist_directory = persist_directory\n        else:\n            if client_settings:\n                # If client_settings is provided with persist_directory specified,\n                # then it is \"in-memory and persisting to disk\" mode.\n                client_settings.persist_directory = (\n                    persist_directory or client_settings.persist_directory\n                )\n                if client_settings.persist_directory is not None:\n                    # Maintain backwards compatibility with chromadb < 0.4.0\n                    major, minor, _ = chromadb.__version__.split(\".\")\n                    if int(major) == 0 and int(minor) < 4:\n                        client_settings.chroma_db_impl = \"duckdb+parquet\"\n\n                _client_settings = client_settings\n            elif persist_directory:\n                # Maintain backwards compatibility with chromadb < 0.4.0\n                major, minor, _ = chromadb.__version__.split(\".\")\n                if int(major) == 0 and int(minor) < 4:\n                    _client_settings = chromadb.config.Settings(\n                        chroma_db_impl=\"duckdb+parquet\",\n                    )\n                else:\n                    _client_settings = chromadb.config.Settings(is_persistent=True)\n                _client_settings.persist_directory = persist_directory\n            else:\n                _client_settings = chromadb.config.Settings()\n            self._client_settings = _client_settings\n            self._client = chromadb.Client(_client_settings)\n            self._persist_directory = (\n                _client_settings.persist_directory or persist_directory\n            )\n\n        self._embedding_function = embedding_function\n        self._collection = self._client.get_or_create_collection(\n            name=collection_name,\n            embedding_function=self._embedding_function.embed_documents\n            if self._embedding_function is not None\n            else None,\n            metadata=collection_metadata,\n        )\n        self.override_relevance_score_fn = relevance_score_fn\n\n    @property\n    def embeddings(self) -> Optional[Embeddings]:\n        return self._embedding_function\n\n    @xor_args((\"query_texts\", \"query_embeddings\"))\n    def __query_collection(\n        self,\n        query_texts: Optional[List[str]] = None,\n        query_embeddings: Optional[List[List[float]]] = None,\n        n_results: int = 4,\n        where: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Query the chroma collection.\"\"\"\n        try:\n            import chromadb  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"Could not import chromadb python package. \"\n                \"Please install it with `pip install chromadb`.\"\n            )\n        return self._collection.query(\n            query_texts=query_texts,\n            query_embeddings=query_embeddings,\n            n_results=n_results,\n            where=where,\n            **kwargs,\n        )\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            ids (Optional[List[str]], optional): Optional list of IDs.\n\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        # TODO: Handle the case where the user doesn't provide ids on the Collection\n        if ids is None:\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n        texts = list(texts)\n        if self._embedding_function is not None:\n            embeddings = self._embedding_function.embed_documents(texts)\n        if metadatas:\n            # fill metadatas with empty dicts if somebody\n            # did not specify metadata for all texts\n            length_diff = len(texts) - len(metadatas)\n            if length_diff:\n                metadatas = metadatas + [{}] * length_diff\n            empty_ids = []\n            non_empty_ids = []\n            for idx, m in enumerate(metadatas):\n                if m:\n                    non_empty_ids.append(idx)\n                else:\n                    empty_ids.append(idx)\n            if non_empty_ids:\n                metadatas = [metadatas[idx] for idx in non_empty_ids]\n                texts_with_metadatas = [texts[idx] for idx in non_empty_ids]\n                embeddings_with_metadatas = (\n                    [embeddings[idx] for idx in non_empty_ids] if embeddings else None\n                )\n                ids_with_metadata = [ids[idx] for idx in non_empty_ids]\n                try:\n                    self._collection.upsert(\n                        metadatas=metadatas,\n                        embeddings=embeddings_with_metadatas,\n                        documents=texts_with_metadatas,\n                        ids=ids_with_metadata,\n                    )\n                except ValueError as e:\n                    if \"Expected metadata value to be\" in str(e):\n                        msg = (\n                            \"Try filtering complex metadata from the document using \"\n                            \"langchain.vectorstore.utils.filter_complex_metadata.\"\n                        )\n                        raise ValueError(e.args[0] + \"\\n\\n\" + msg)\n                    else:\n                        raise e\n            if empty_ids:\n                texts_without_metadatas = [texts[j] for j in empty_ids]\n                embeddings_without_metadatas = (\n                    [embeddings[j] for j in empty_ids] if embeddings else None\n                )\n                ids_without_metadatas = [ids[j] for j in empty_ids]\n                self._collection.upsert(\n                    embeddings=embeddings_without_metadatas,\n                    documents=texts_without_metadatas,\n                    ids=ids_without_metadatas,\n                )\n        else:\n            self._collection.upsert(\n                embeddings=embeddings,\n                documents=texts,\n                ids=ids,\n            )\n        return ids\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Run similarity search with Chroma.\n\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\n        docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\n        return [doc for doc, _ in docs_and_scores]\n\n    def similarity_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to embedding vector.\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List of Documents most similar to the query vector.\n        \"\"\"\n        results = self.__query_collection(\n            query_embeddings=embedding, n_results=k, where=filter\n        )\n        return _results_to_docs(results)\n\n    def similarity_search_by_vector_with_relevance_scores(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"\n        Return docs most similar to embedding vector and similarity score.\n\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        results = self.__query_collection(\n            query_embeddings=embedding, n_results=k, where=filter\n        )\n        return _results_to_docs_and_scores(results)\n\n    def similarity_search_with_score(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Run similarity search with Chroma with distance.\n\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        if self._embedding_function is None:\n            results = self.__query_collection(\n                query_texts=[query], n_results=k, where=filter\n            )\n        else:\n            query_embedding = self._embedding_function.embed_query(query)\n            results = self.__query_collection(\n                query_embeddings=[query_embedding], n_results=k, where=filter\n            )\n\n        return _results_to_docs_and_scores(results)\n\n    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n        \"\"\"\n        The 'correct' relevance function\n        may differ depending on a few things, including:\n        - the distance / similarity metric used by the VectorStore\n        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)\n        - embedding dimensionality\n        - etc.\n        \"\"\"\n        if self.override_relevance_score_fn:\n            return self.override_relevance_score_fn\n\n        distance = \"l2\"\n        distance_key = \"hnsw:space\"\n        metadata = self._collection.metadata\n\n        if metadata and distance_key in metadata:\n            distance = metadata[distance_key]\n\n        if distance == \"cosine\":\n            return self._cosine_relevance_score_fn\n        elif distance == \"l2\":\n            return self._euclidean_relevance_score_fn\n        elif distance == \"ip\":\n            return self._max_inner_product_relevance_score_fn\n        else:\n            raise ValueError(\n                \"No supported normalization function\"\n                f\" for distance metric of type: {distance}.\"\n                \"Consider providing relevance_score_fn to Chroma constructor.\"\n            )\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n\n        results = self.__query_collection(\n            query_embeddings=embedding,\n            n_results=fetch_k,\n            where=filter,\n            include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"],\n        )\n        mmr_selected = maximal_marginal_relevance(\n            np.array(embedding, dtype=np.float32),\n            results[\"embeddings\"][0],\n            k=k,\n            lambda_mult=lambda_mult,\n        )\n\n        candidates = _results_to_docs(results)\n\n        selected_results = [r for i, r in enumerate(candidates) if i in mmr_selected]\n        return selected_results\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            query: Text to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        if self._embedding_function is None:\n            raise ValueError(\n                \"For MMR search, you must specify an embedding function on\" \"creation.\"\n            )\n\n        embedding = self._embedding_function.embed_query(query)\n        docs = self.max_marginal_relevance_search_by_vector(\n            embedding, k, fetch_k, lambda_mult=lambda_mult, filter=filter\n        )\n        return docs\n\n    def delete_collection(self) -> None:\n        \"\"\"Delete the collection.\"\"\"\n        self._client.delete_collection(self._collection.name)\n\n    def get(\n        self,\n        ids: Optional[OneOrMany[ID]] = None,\n        where: Optional[Where] = None,\n        limit: Optional[int] = None,\n        offset: Optional[int] = None,\n        where_document: Optional[WhereDocument] = None,\n        include: Optional[List[str]] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Gets the collection.\n\n        Args:\n            ids: The ids of the embeddings to get. Optional.\n            where: A Where type dict used to filter results by.\n                   E.g. `{\"color\" : \"red\", \"price\": 4.20}`. Optional.\n            limit: The number of documents to return. Optional.\n            offset: The offset to start returning results from.\n                    Useful for paging results with limit. Optional.\n            where_document: A WhereDocument type dict used to filter by the documents.\n                            E.g. `{$contains: {\"text\": \"hello\"}}`. Optional.\n            include: A list of what to include in the results.\n                     Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n                     Ids are always included.\n                     Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n        \"\"\"\n        kwargs = {\n            \"ids\": ids,\n            \"where\": where,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"where_document\": where_document,\n        }\n\n        if include is not None:\n            kwargs[\"include\"] = include\n\n        return self._collection.get(**kwargs)\n\n    def persist(self) -> None:\n        \"\"\"Persist the collection.\n\n        This can be used to explicitly persist the data to disk.\n        It will also be called automatically when the object is destroyed.\n        \"\"\"\n        if self._persist_directory is None:\n            raise ValueError(\n                \"You must specify a persist_directory on\"\n                \"creation to persist the collection.\"\n            )\n        import chromadb\n\n        # Maintain backwards compatibility with chromadb < 0.4.0\n        major, minor, _ = chromadb.__version__.split(\".\")\n        if int(major) == 0 and int(minor) < 4:\n            self._client.persist()\n\n    def update_document(self, document_id: str, document: Document) -> None:\n        \"\"\"Update a document in the collection.\n\n        Args:\n            document_id (str): ID of the document to update.\n            document (Document): Document to update.\n        \"\"\"\n        text = document.page_content\n        metadata = document.metadata\n        if self._embedding_function is None:\n            raise ValueError(\n                \"For update, you must specify an embedding function on creation.\"\n            )\n        embeddings = self._embedding_function.embed_documents([text])\n\n        self._collection.update(\n            ids=[document_id],\n            embeddings=embeddings,\n            documents=[text],\n            metadatas=[metadata],\n        )\n\n    @classmethod\n    def from_texts(\n        cls: Type[Chroma],\n        texts: List[str],\n        embedding: Optional[Embeddings] = None,\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        client: Optional[chromadb.Client] = None,\n        collection_metadata: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> Chroma:\n        \"\"\"Create a Chroma vectorstore from a raw documents.\n\n        If a persist_directory is specified, the collection will be persisted there.\n        Otherwise, the data will be ephemeral in-memory.\n\n        Args:\n            texts (List[str]): List of texts to add to the collection.\n            collection_name (str): Name of the collection to create.\n            persist_directory (Optional[str]): Directory to persist the collection.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n            collection_metadata (Optional[Dict]): Collection configurations.\n                                                  Defaults to None.\n\n        Returns:\n            Chroma: Chroma vectorstore.\n        \"\"\"\n        chroma_collection = cls(\n            collection_name=collection_name,\n            embedding_function=embedding,\n            persist_directory=persist_directory,\n            client_settings=client_settings,\n            client=client,\n            collection_metadata=collection_metadata,\n            **kwargs,\n        )\n        chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n        return chroma_collection\n\n    @classmethod\n    def from_documents(\n        cls: Type[Chroma],\n        documents: List[Document],\n        embedding: Optional[Embeddings] = None,\n        ids: Optional[List[str]] = None,\n        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,\n        persist_directory: Optional[str] = None,\n        client_settings: Optional[chromadb.config.Settings] = None,\n        client: Optional[chromadb.Client] = None,  # Add this line\n        collection_metadata: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> Chroma:\n        \"\"\"Create a Chroma vectorstore from a list of documents.\n\n        If a persist_directory is specified, the collection will be persisted there.\n        Otherwise, the data will be ephemeral in-memory.\n\n        Args:\n            collection_name (str): Name of the collection to create.\n            persist_directory (Optional[str]): Directory to persist the collection.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            documents (List[Document]): List of documents to add to the vectorstore.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n            collection_metadata (Optional[Dict]): Collection configurations.\n                                                  Defaults to None.\n\n        Returns:\n            Chroma: Chroma vectorstore.\n        \"\"\"\n        texts = [doc.page_content for doc in documents]\n        metadatas = [doc.metadata for doc in documents]\n        return cls.from_texts(\n            texts=texts,\n            embedding=embedding,\n            metadatas=metadatas,\n            ids=ids,\n            collection_name=collection_name,\n            persist_directory=persist_directory,\n            client_settings=client_settings,\n            client=client,\n            collection_metadata=collection_metadata,\n            **kwargs,\n        )\n\n    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:\n        \"\"\"Delete by vector IDs.\n\n        Args:\n            ids: List of ids to delete.\n        \"\"\"\n        self._collection.delete(ids=ids)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:09:45.692537Z","iopub.execute_input":"2023-08-31T16:09:45.692925Z","iopub.status.idle":"2023-08-31T16:09:45.769625Z","shell.execute_reply.started":"2023-08-31T16:09:45.692892Z","shell.execute_reply":"2023-08-31T16:09:45.768580Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\ndevice_type=\"cuda\"\nshow_sources =True\nEMBEDDING_MODEL_NAME = 'moka-ai/m3e-base'\nlogging.info(f\"Running on: {device_type}\")\nlogging.info(f\"Display Source Documents set to: {show_sources}\")\nlogger.error(f\"EMBEDDING_MODEL_NAME={EMBEDDING_MODEL_NAME}\")\nembeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n\n# uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n# load the vectorstore\ndb = MyChroma(\n        persist_directory=PERSIST_DIRECTORY,\n        embedding_function=embeddings,\n        client_settings=CHROMA_SETTINGS,\n        collection_metadata={\"hnsw:space\": \"ip\"}, # 重要add 20230831 将默认的L2 distance换成 cosine similarity\n          \n)\n#retriever = db.as_retriever()\nscore_threshold = 0.6\nretriever = db.as_retriever(\n    search_type=\"similarity_score_threshold\", \n    search_kwargs={\"k\":3, \"score_threshold\":score_threshold}\n    )","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:09:58.827478Z","iopub.execute_input":"2023-08-31T16:09:58.827859Z","iopub.status.idle":"2023-08-31T16:10:04.887349Z","shell.execute_reply.started":"2023-08-31T16:09:58.827826Z","shell.execute_reply":"2023-08-31T16:10:04.886279Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\nload INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"code","source":"#db","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"使用如下信息回答问题. 如果不知道答案,\\\n    就回答不知道，不要编造答案.\n\n    {context}\n\n    {history}\n    问: {question}\n    答:\"\"\"\n\nprompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\nmemory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\nlogger.error(isinstance(memory,list)) # memory 不是 list\n#print(memory)\n#llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\nllm = ChatGLM()\nqa = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n    )\n    # Interactive questions and answers\ncnt = 0","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:10:17.368321Z","iopub.execute_input":"2023-08-31T16:10:17.368733Z","iopub.status.idle":"2023-08-31T16:10:17.378518Z","shell.execute_reply.started":"2023-08-31T16:10:17.368675Z","shell.execute_reply":"2023-08-31T16:10:17.377507Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# 注意  打印history的语句在chatglm_llm.py最下方 那里也可以调节history的取值 我现在取值history[-2:0]\nwhile cnt == 0:\n        #cnt += 1\n        query = input(\"输入问题:\\n\")\n        #query = \"how long is the period of united states president?\"\n        #query = \"SSE报销预览打印错误怎么办\"\n        if query == \"exit\":\n            break\n        # Get the answer from the chain\n        res = qa(query)\n        print(res)\n        print(\"*******\")\n        answer, docs = res[\"result\"], res[\"source_documents\"]\n\n        # Print the result\n        print(\"\\n\\n> Question:\")\n        print(query)\n        print(\"\\n> Answer:\")\n        print(answer)\n\n        if show_sources:  # this is a flag that you can set to disable showing answers.\n            # # Print the relevant sources used for the answer\n            print(\"----------------------------------SOURCE DOCUMENTS INFO---------------------------\")\n            print(f\"score_threshold={score_threshold}\")\n            for document in docs:\n                print(\"\\n> [来源文档]: \" + document.metadata[\"source\"] )\n                print(\"> [cosine相似度得分 (0-1之间越高越相似)]:\" + str(1.0-document.metadata[\"distances\"]) )\n                print(\">[文档片段]:\" + document.page_content)\n            if len(docs)==0:\n                print(\"没有从知识库中搜索到关联信息 上面的答案answer需要重新组织 很可能是不准确的\")\n            #print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-08-31T16:10:21.080113Z","iopub.execute_input":"2023-08-31T16:10:21.080483Z","iopub.status.idle":"2023-08-31T16:12:44.733018Z","shell.execute_reply.started":"2023-08-31T16:10:21.080451Z","shell.execute_reply":"2023-08-31T16:12:44.731892Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdin","text":"输入问题:\n sse\n"},{"name":"stderr","text":"\u001b[32m2023-08-31 16:10:36.101\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m66\u001b[0m - \u001b[31m\u001b[1mupdated_history[-1]=('使用如下信息回答问题. 如果不知道答案,    就回答不知道，不要编造答案.\\n\\n    1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n    \\n    问: sse\\n    答:', '抱歉，我不太明白您的问题是什么，\"sse\" 可能是一个缩写词，但需要更多的上下文才能回答您的问题。请提供更多信息，以便我更好地回答您的问题。')\u001b[0m\n\u001b[32m2023-08-31 16:10:36.103\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m70\u001b[0m - \u001b[31m\u001b[1monly use history[-1:],history=[('国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n    \\n    问: sse\\n    答:', '抱歉，我不太明白您的问题是什么，\"sse\" 可能是一个缩写词，但需要更多的上下文才能回答您的问题。请提供更多信息，以便我更好地回答您的问题。')]\nlen(self.history)=1\u001b[0m\n\u001b[32m2023-08-31 16:10:36.104\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m71\u001b[0m - \u001b[31m\u001b[1mchar_len_total =570\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"('国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n    \\n    问: sse\\n    答:', '抱歉，我不太明白您的问题是什么，\"sse\" 可能是一个缩写词，但需要更多的上下文才能回答您的问题。请提供更多信息，以便我更好地回答您的问题。')\n{'query': 'sse', 'result': '抱歉，我不太明白您的问题是什么，\"sse\" 可能是一个缩写词，但需要更多的上下文才能回答您的问题。请提供更多信息，以便我更好地回答您的问题。', 'source_documents': [Document(page_content='1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt', 'distances': 0.17582112550735474}), Document(page_content='1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt', 'distances': 0.17582112550735474}), Document(page_content='1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt', 'distances': 0.17582112550735474})]}\n*******\n\n\n> Question:\nsse\n\n> Answer:\n抱歉，我不太明白您的问题是什么，\"sse\" 可能是一个缩写词，但需要更多的上下文才能回答您的问题。请提供更多信息，以便我更好地回答您的问题。\n----------------------------------SOURCE DOCUMENTS INFO---------------------------\nscore_threshold=0.6\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt\n> [cosine相似度得分 (0-1之间越高越相似)]:0.8241788744926453\n>[文档片段]:1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt\n> [cosine相似度得分 (0-1之间越高越相似)]:0.8241788744926453\n>[文档片段]:1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt\n> [cosine相似度得分 (0-1之间越高越相似)]:0.8241788744926453\n>[文档片段]:1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n2. 由于发票核验和录入都失败，我们需要怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n3. 发票核验和录入都失败了，我们该如何解决？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n4. 当发票核验和录入都失败时，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n5. 发票核验失败，发票录入也失败，怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n6. 如果发票核验和录入都失败，我们该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n7. 发票核验失败且录入失败，我们该如何应对？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"输入问题:\n 发票核验和录入都失败了，应该怎么办\n"},{"name":"stderr","text":"\u001b[32m2023-08-31 16:11:53.890\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m66\u001b[0m - \u001b[31m\u001b[1mupdated_history[-1]=('使用如下信息回答问题. 如果不知道答案,    就回答不知道，不要编造答案.\\n\\n    2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n    Human: sse\\nAI: 抱歉，我不太明白您的问题是什么，\"sse\" 可能是一个缩写词，但需要更多的上下文才能回答您的问题。请提供更多信息，以便我更好地回答您的问题。\\n    问: 发票核验和录入都失败了，应该怎么办\\n    答:', '如果发票核验和录入都失败了，您可以尝试以下步骤来解决问题：\\n\\n1. 检查发票信息：检查发票上的信息是否填写完整，如抬头、地址、电话、税号等，确保所有信息都正确无误。\\n2. 检查国税局的网站：检查国税局的网站是否正常运行，如果网站无法正常访问，可以尝试在另一个浏览器或电脑上访问。\\n3. 检查发票系统：检查发票系统是否正常运行，如果系统无法正常运行，可以尝试稍后再试。\\n4. 联系发票系统管理员：如果发票系统出现故障，可以联系发票系统管理员，寻求帮助和解决方案。\\n5. 重新开具发票：如果以上步骤都无法解决问题，可以考虑重新开具发票。\\n\\n请注意，如果发票存在问题，您需要及时联系相关税务部门或发票系统管理员，以确保税务事宜的顺利进行。')\u001b[0m\n\u001b[32m2023-08-31 16:11:53.891\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m70\u001b[0m - \u001b[31m\u001b[1monly use history[-1:],history=[('E只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n    Human: sse\\nAI: 抱歉，我不太明白您的问题是什么，\"sse\" 可能是一个缩写词，但需要更多的上下文才能回答您的问题。请提供更多信息，以便我更好地回答您的问题。\\n    问: 发票核验和录入都失败了，应该怎么办\\n    答:', '如果发票核验和录入都失败了，您可以尝试以下步骤来解决问题：\\n\\n1. 检查发票信息：检查发票上的信息是否填写完整，如抬头、地址、电话、税号等，确保所有信息都正确无误。\\n2. 检查国税局的网站：检查国税局的网站是否正常运行，如果网站无法正常访问，可以尝试在另一个浏览器或电脑上访问。\\n3. 检查发票系统：检查发票系统是否正常运行，如果系统无法正常运行，可以尝试稍后再试。\\n4. 联系发票系统管理员：如果发票系统出现故障，可以联系发票系统管理员，寻求帮助和解决方案。\\n5. 重新开具发票：如果以上步骤都无法解决问题，可以考虑重新开具发票。\\n\\n请注意，如果发票存在问题，您需要及时联系相关税务部门或发票系统管理员，以确保税务事宜的顺利进行。')]\nlen(self.history)=1\u001b[0m\n\u001b[32m2023-08-31 16:11:53.892\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mmy_chatglm_llm\u001b[0m:\u001b[36m_call\u001b[0m:\u001b[36m71\u001b[0m - \u001b[31m\u001b[1mchar_len_total =816\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"('E只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n\\n    Human: sse\\nAI: 抱歉，我不太明白您的问题是什么，\"sse\" 可能是一个缩写词，但需要更多的上下文才能回答您的问题。请提供更多信息，以便我更好地回答您的问题。\\n    问: 发票核验和录入都失败了，应该怎么办\\n    答:', '如果发票核验和录入都失败了，您可以尝试以下步骤来解决问题：\\n\\n1. 检查发票信息：检查发票上的信息是否填写完整，如抬头、地址、电话、税号等，确保所有信息都正确无误。\\n2. 检查国税局的网站：检查国税局的网站是否正常运行，如果网站无法正常访问，可以尝试在另一个浏览器或电脑上访问。\\n3. 检查发票系统：检查发票系统是否正常运行，如果系统无法正常运行，可以尝试稍后再试。\\n4. 联系发票系统管理员：如果发票系统出现故障，可以联系发票系统管理员，寻求帮助和解决方案。\\n5. 重新开具发票：如果以上步骤都无法解决问题，可以考虑重新开具发票。\\n\\n请注意，如果发票存在问题，您需要及时联系相关税务部门或发票系统管理员，以确保税务事宜的顺利进行。')\n{'query': '发票核验和录入都失败了，应该怎么办', 'result': '如果发票核验和录入都失败了，您可以尝试以下步骤来解决问题：\\n\\n1. 检查发票信息：检查发票上的信息是否填写完整，如抬头、地址、电话、税号等，确保所有信息都正确无误。\\n2. 检查国税局的网站：检查国税局的网站是否正常运行，如果网站无法正常访问，可以尝试在另一个浏览器或电脑上访问。\\n3. 检查发票系统：检查发票系统是否正常运行，如果系统无法正常运行，可以尝试稍后再试。\\n4. 联系发票系统管理员：如果发票系统出现故障，可以联系发票系统管理员，寻求帮助和解决方案。\\n5. 重新开具发票：如果以上步骤都无法解决问题，可以考虑重新开具发票。\\n\\n请注意，如果发票存在问题，您需要及时联系相关税务部门或发票系统管理员，以确保税务事宜的顺利进行。', 'source_documents': [Document(page_content='2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt', 'distances': 0.1195141077041626}), Document(page_content='2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt', 'distances': 0.1195141077041626}), Document(page_content='2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试', metadata={'source': '/kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt', 'distances': 0.1195141077041626})]}\n*******\n\n\n> Question:\n发票核验和录入都失败了，应该怎么办\n\n> Answer:\n如果发票核验和录入都失败了，您可以尝试以下步骤来解决问题：\n\n1. 检查发票信息：检查发票上的信息是否填写完整，如抬头、地址、电话、税号等，确保所有信息都正确无误。\n2. 检查国税局的网站：检查国税局的网站是否正常运行，如果网站无法正常访问，可以尝试在另一个浏览器或电脑上访问。\n3. 检查发票系统：检查发票系统是否正常运行，如果系统无法正常运行，可以尝试稍后再试。\n4. 联系发票系统管理员：如果发票系统出现故障，可以联系发票系统管理员，寻求帮助和解决方案。\n5. 重新开具发票：如果以上步骤都无法解决问题，可以考虑重新开具发票。\n\n请注意，如果发票存在问题，您需要及时联系相关税务部门或发票系统管理员，以确保税务事宜的顺利进行。\n----------------------------------SOURCE DOCUMENTS INFO---------------------------\nscore_threshold=0.6\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt\n> [cosine相似度得分 (0-1之间越高越相似)]:0.8804858922958374\n>[文档片段]:2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt\n> [cosine相似度得分 (0-1之间越高越相似)]:0.8804858922958374\n>[文档片段]:2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n\n> [来源文档]: /kaggle/working/localGPT/SOURCE_DOCUMENTS/yunguan2.txt\n> [cosine相似度得分 (0-1之间越高越相似)]:0.8804858922958374\n>[文档片段]:2. 您在发票录入时，发现出现了“查无此票”的提示信息，您该如何处理？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n3. 根据您提供的信息，发票录入后出现了“查无此票”的错误提示，您能提供一些具体的解决方案吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n4. 发票录入时遇到“查无此票”问题，您是如何处理的？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n5. 当您在发票录入时发现“查无此票”时，您会采取哪些措施来解决此问题？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n6. 发票录入后，出现了“查无此票”的错误提示，您能提供一些实际可行的解决方法吗？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n7. 当您在发票录入时遇到“查无此票”问题，您会尝试哪些方法来解决呢？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n发票核验失败 发票录入失败。怎么办？答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n1. 发票核验和录入都失败了，应该怎么办？ 答：您好，当天的开票在税务局还未备案，无法查验，各地税务局备案时间不一，SSE只是调用国家税务局发票系统的核验接口，并返回国税局的核验结果。建议您开票后的一个工作日再试\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"输入问题:\n exit\n"}]}]}